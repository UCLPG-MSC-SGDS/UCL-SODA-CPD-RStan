[["index.html", "Introduction to Bayesian Inference and Modelling (June 2025) Welcome Timetable, Schedule &amp; Location My Contact Details", " Introduction to Bayesian Inference and Modelling (June 2025) Welcome UCL Social Data Institute (SODA) is hosting a five-day course on Bayesian inference. The aim is to introduce academics and professional data analysts to the basics of Bayesian inference using RStudio and Stan. The course atmosphere will be extremely friendly and supportive, with the goal of teaching the fundamentals of Bayesian inference in Stan to participants from diverse backgrounds—including industry and research fields such as population health, social sciences, disaster-risk reduction, and many more. By the end of the workshop, the you should be able to: Acquire the foundation and advanced knowledge on key principles of statistical modelling within a Bayesian framework; Be able to perform inferential statistics on spatial and non-spatial data to carry out hypothesis testing for evidence-based research using the diverse types of regression-based models from a Bayesian framework; Be able to perform spatial risk prediction for areal data as well as quantify levels of uncertainty using exceedance probabilities; Acquire new programming language skills such as Stan (interfaced with RStudio). Timetable, Schedule &amp; Location The course will cover the following topics: Date Downloadables Topics Not Applicable Installation of R, RStudio &amp; Stan 09/06/2025 [Slides]; [Dataset] Introduction to Probability Distributions 10/06/2025 [Slides]; N/A Introduction to Bayesian Inference 11/06/2025 [Slides]; [Dataset] Bayesian Generalised Linear Models 12/06/2025 [Slides]; [Dataset] Bayesian Hierarchical Regression Models 13/06/2025 [Slides]; [Dataset] Spatial Bayesian Risk Modelling Solutions: [Day 1] | [Day 2] | [Day 3] | [Day 4] | [Day 5] Throughout the course, each day will consist of a lecture, a walkthrough demonstration and computer seminar session. Here is the information on the programme’s schedule: Times Format 10:30am-12:00pm (1h30mins) Lecture 12:00pm-01:00pm (1h) Lunch Break 01:00pm-02:00pm (1h) Live Demonstration Walk-through 02:00pm-02:15pm (0h15mins) Short Break 02:15pm-04:15pm (2h) Computer Practical Session 04:15pm-04:30pm (0h15mins) Wrap-up 04:30pm Close All sessions are delivered in-person at the location: Room G04 Seminar Room 2 Charles Bell House (University College London), 43–45 Foley Street, London, W1W 7TY View UCL Map | View Google Map IMPORTANT NOTE: Please bring your own laptops with you to benefit from the course specifically for the walkthrough and computer practicals. My Contact Details Anwar Musah Lecturer in Social &amp; Geographic Data Science UCL Department of Geography Room 115 (First Floor) North West Wing Building, Gower Street, London, WC1E 6BT Email: a.musah@ucl.ac.uk; Telephone: +44 (0)748 279 0776 LinkedIn | UCL Profile "],["reading-list.html", "Reading List Day 1: Introduction to Probability Distributions Day 2: Introduction to Bayesian Inference Day 3: Bayesian Generalised Linear Models (GLMs) Day 4: Bayesian Hierarchical Regression Models Day 5: Bayesian Spatial Modelling for Areal Data in Stan", " Reading List Contact me via email a.musah@ucl.ac.uk if you are having problems securing one of these recommended books. Check these downloadable ‘Easter Eggs’ in this Google Drive Repository [LINK]. Day 1: Introduction to Probability Distributions Book: [Theory] Slater, M. (2022). Bayesian Methods in Statistics: From Concepts to Practice. Chapters 2: Probability Distributions. Pages 24-45. Book: [Theory] Donovan, T.M., &amp; Mickey, R.M. (2019). Bayesian Statistics for Beginners: A Step-by-Step Approach. Chapters 1: Introduction to Probability. Pages 3-29. Book: [Theory] Donovan, T.M., &amp; Mickey, R.M. (2019). Bayesian Statistics for Beginners: A Step-by-Step Approach. Chapters 3: Probability Functions. Pages 87-132. Article: [Stan Programming] Carpenter, B., Gelman, A., et al (2019). Stan: A Probabilistic Programming Language. J Stat Soft. DOI: 10.18637/jss.v076.i01. Day 2: Introduction to Bayesian Inference Book: [Theory] Donovan, T.M., &amp; Mickey, R.M. (2019). Bayesian Statistics for Beginners: A Step-by-Step Approach. Chapters 2: Bayes’ Theorem and Bayesian Inference. Pages 3-29. Day 3: Bayesian Generalised Linear Models (GLMs) Book: [Theory] Slater, M. (2022). Bayesian Methods in Statistics: From Concepts to Practice. Chapters 5: General Models. Pages 114-151. Article: [Theory] Baldwin, S.A., &amp; Larson, M.J. (2017). An introduction to using Bayesian linear regression with clinical data. Behavior Research and Therapy. 98:58-75. DOI: 10.1016/j.brat.2016.12.016. Book: [Theory] Gelman, A et al. (2014). Bayesian Data Analysis (3rd Edition). Chapters 14: Introduction to Regression Models. Pages 353-378. Book: [Stan Programming] Lambert, B. (2018). A Student’s Guide to Bayesian Statistics. Chapters 18: Linear Regression Models. Pages 453-466. Day 4: Bayesian Hierarchical Regression Models Article: [Stan Programming] Sorensen, T., &amp; Vasishth, S. (2016). Bayesian linear mixed models using Stan: A tutorial for psychologists, linguists, and cognitive scientists. Tutorials in Quantitative Methods for Psychology. 12(3):175-200. DOI: 10.20982/tqmp.12.3.p175 Book: [Theory] Gelman, A et al. (2014). Bayesian Data Analysis (3rd Edition).Chapters 15: Hierarchical Linear Models. Pages 381-402. Day 5: Bayesian Spatial Modelling for Areal Data in Stan Article: [Application] Li, L. et al (2022). An ecological study exploring the geospatial associations between socioeconomic deprivation and fire-related dwelling casualties in the England (2010–2019). Applied Geography. 144(1027718). DOI: 10.1016/j.apgeog.2022.102718. Article: [Application] Gomez, M.J. et al (2023). Bayesian spatial modeling of childhood overweight and obesity prevalence in Costa Rica. BMC Public Health. 23(651). DOI:10.1186/s12889-023-15486-1 Article: [Theory] Morris, M. et al (2019). Bayesian hierarchical spatial models: Implementing the Besag York Mollié model in stan. Spatial and Spatio-temporal Epidemiology. 31(100301). DOI: 10.1016/j.sste.2019.100301 Online Tutorials: [Stan Programming] Morris, M. et al (2019). Spatial Models in Stan: Intrinsic Auto-Regressive Models for Areal Data. URL: https://mc-stan.org/users/documentation/case-studies/icar_stan.html Article: [History] Besag, J. (1974). Spatial interaction and the statistical analysis of lattice systems. Journal of the Royal Statistical Society. Series B (Methodological) (1974): 192-236. Article: [History] Besag, J. &amp; Kooperberg, K. (1995) “On conditional and intrinsic autoregression. Biometrika. 733-746. Article: [History] Riebler, A., et al (2016). An intuitive Bayesian spatial model for disease mapping that accounts for scaling. Statistical methods in medical research. 25(4): 1145-1165. DOI: 10.1177/0962280216660421 "],["installation-of-r-rstudio-stan.html", "1 Installation of R, RStudio &amp; Stan 1.1 What is Stan? 1.2 Installation of R &amp; RStudio 1.3 Installation of rstan (or Stan) &amp; other important r-packages", " 1 Installation of R, RStudio &amp; Stan 1.1 What is Stan? Stan is an interface for several statistical software packages (e.g., RStudio, Python, Julia, Stata, and MATLAB) which allows the user to perform state-of-the-art statistical modelling within a Bayesian framework. For R users, the package is called rstan which interfaces Stan and RStudio. The focus will be solely on Stan and RStudio. We will show you how one can develop and compile Stan scripts for Bayesian inference through RStudio to perform basic parameter estimation, as well as a wide range of regression-based techniques starting with the simplest univariable linear models and its different families (logistic and Poisson) to the more advanced multivariable spatial risk models. Before all that, let us install the appropriate software and their latest version (as of June 3rd 2025). The next section will guide you through the installation process. 1.2 Installation of R &amp; RStudio This section takes you through the installation process for R (Base) and RStudio on MAC and Windows. If you are a MAC user, please jump to section 1.2.1 If you are a Windows user, please jump to section 1.2.2 1.2.1 Installation process for MAC users You will need to have the following software installed for the rstan package to work on MAC. It is recommended to have the latest version of R and RStudio R (version 4.5.0) and RStudio (version 2025.05.0+496) XQuartz (version 2.8.5) XCode (version 16.4) GNU Fortran (version 12.2) [1] Installation of R (4.4.2) and RStudio (2024.12.0-427) on MAC: For R (Base), please ensure you have installed the correct version for your MAC (Mac Intel) or MAC (Apple silicon M1, M2 or M3) OS. OS User type R (Base) RStudio Desktop MAC (Intel) R-4.5.0-x86_64.pkg RStudio-2025.05.0-496.dmg MAC (M1, M2 or M3) R-4.5.0-arm64.pkg RStudio-2025.05.0-496.dmg Download the correct version of R (Base) for your system. Double-click on the downloaded file (i.e., R-4.5.0-x86_64.pkg or R-4.5.0-arm64.pkg) and follow the steps to complete the installation. Now, we can download the file (i.e., .dmg) for RStudio from the link provided in the above table. Double-click the downloaded file (i.e., RStudio-2025.05.0-496.dmg) and then drag and drop the RStudio icon into the Applications folder to complete the installation. [2] Installation of XQuartz (2.8.5): Some functions in R (Base) and Stan require some of the libraries from XQuartz in order to function smoothly on your MAC OS. Download the latest version of XQuartz (XQuartz-2.8.5.pkg) by clicking on this LINK and simply complete the installation process by following the steps on your system. [3] Installation of XCode (16.4): Some functions in R (Base) and Stan require some of the external developer tools from the XCode application to function properly on your MAC OS. Go to the App Store application and get the XCode app downloaded on to your system by clicking this LINK. Once it has downloaded, you can click on the “OPEN” button to verify it’s been downloaded. A window will prompt you to complete the installation. [4] GNU Fortran (version 12.2): R (Base) and some packages require the GNU Fortran 12.2 compiler in order to function smoothly on your MAC OS. Download the latest version of GNU Fortran 12.2 (gfortran-12.2-universal.pkg) by clicking on this LINK and simply complete the installation process by following the steps on your system. IMPORTANT NOTE: The above four steps should complete the installation process for R and RStudio on MAC. 1.2.2 Installation process for Windows users You will need to have the following software installed for the rstan package to work on Windows. R (version 4.5.0) RTools45 (version 4.5.0) RStudio (version 2025.05.0+496) [1] Installation of R (4.3.2) and RStudio (2023.06.0-421) on Windows: OS User type R (Base) RStudio Desktop Windows R-4.5.0-win.exe RStudio-2025.05.0-496.exe Download the file for R-4.5.0-win.exe attached in the table above. Double-click the downloaded file (i.e., R-4.5.0-win.exe) and follow the steps to complete the installation on your system. Now, we can download the file (i.e., .exe) for RStudio from the link provided in the above table. Double-click the downloaded file (i.e., RStudio-2025.05.0-496.exe) and follow the steps from the installer to complete the installation. [2] Installation of Rtools 4.5.0 For Windows users, after you have completed the installation for R (Base) and RStudio, you are required to install the RTools45 package as it contains some libraries and developer tools for R function properly. Download the latest version of RTools45 by clicking on this LINK to initiate the download of the Rtools45 installer. Double-click the downloaded rtools45-6536-6492.exe file and follow the steps to complete the installation. IMPORTANT NOTE: The above two steps should complete the installation process for R and RStudio on Windows. 1.3 Installation of rstan (or Stan) &amp; other important r-packages When opening the RStudio application on your Windows or MAC PC. You will be greeted with its interface. The window is usual split into three panels: 1.) R Console, 2.) Environments and 3.) Files, Help, Outputs etc., The above section is the Menu Bar. You can access other functions for saving, editing, and opening a new R and Stan script files for writing and compiling codes. Let us opening a new R script by clicking on the File &gt; New File &gt; R Script. This should open a new script file titled “Untitled 1”. Now we are going to latest version of rstan 2.36.0.9000 (as of June 3rd 2025). Using the install.packages() function, we can finally install this package. You can use the code chunk below: install.packages(&quot;rstan&quot;, repos = c(&#39;https://stan-dev.r-universe.dev&#39;, getOption(&quot;repos&quot;))) After installation, use the following code chunk to test if its work: example(stan_model, package = &quot;rstan&quot;, run.dontrun = TRUE) You will first see some gibberish running through your console - don’t be alarmed - it means that its working. You will know rstan has been successfully installed, and working, when you see some iterations for four chains displayed in console. You will also see the objects fit, fit2, mod and stancode stored in the Environments panel when its done. This completes the installation process for rstan. Finally, we install other relevant R-packages needed for these sessions. It includes the following: sf: “Simply Features” package that allows the user to load shapefiles into RStudio’s memory. tmap: this package gives access to various functions for users to generate maps. stars: this package for handling SpatioTemporal Arrays, Raster and vector data. SpatialEpi: grants access to the expected() function needed for calculating expected numbers. geostan: grants access to further functions that we need to compute the adjacency matrix that can be handled in Stan. We will use the two functions shape2mat() and prep_icar_data() to create the adjacency matrix as nodes and edges. tidybayes: grants access to further functions for managing posterior estimates. We will need it calculating the exceedance probabilities. Note that is loaded alongside tidyverse and dplyr packages. bayesplot: grants access to further functions for plot posterior estimates from Bayesian models. loo: this package allows the user to perform model validation and comparison tidyverse: grants access to ‘streamlined’ codes and functions for simplified data management in R beyond those provided with Base R codes. dplyr: grants further access to more ‘streamlined’ codes and functions for simplified data management in R beyond those provided with Base R codes. install.packages(&quot;sf&quot;) install.packages(&quot;tmap&quot;) install.packages(&quot;Stars&quot;) install.packages(&quot;SpatialEpi&quot;) install.packages(&quot;geostan&quot;) install.packages(&quot;tidybayes&quot;) install.packages(&quot;bayesplot&quot;) install.packages(&quot;loo&quot;) install.packages(&quot;tidyverse&quot;) install.packages(&quot;dplyr&quot;) This concludes the installation section and sets you computer up for the course, if you encounter any problems please contact me. "],["introduction-to-stan-programming.html", "2 Introduction to Stan Programming 2.1 Introduction 2.2 About Stan Programming 2.3 Tasks", " 2 Introduction to Stan Programming 2.1 Introduction 2.1.1 Lectures (Length: 1:55:55) [Watch on YouTube] 2.1.2 Learning outcomes Today’s session aims to introduce you to the basic Stan programming etiquette for Bayesian analysis in RStudio using Stan as an Interface, and producing output and interpreting it’s results. By the end of this session, you should be able to perform the following: Getting acquainted with Stan and learning the basic programming etiquette of Stan - this will include simulating and estimation of parameters. Know how to write and compile various probability distributions for simple parameters (i.e., mean, standard deviation, a proportion etc.,) in Stan. Know how to compile the results into RStudio for use, interpretation and visualisation. After the demonstration session is delivered in the first 1-hour of the practical - you can use the computer practical session to try the tutorials yourself by following the instructions as well as trying out the tasks. 2.1.3 Demonstration (Length: 1:08:38) [Watch on YouTube] 2.1.4 Setting up the work directory Since, this is our first practical lesson for Day 1, let us create a new folder CPD-course at the desktop location of our computer. Now, create a sub folder called “Day 1” within the CPD-course folder. Here, we will store all our R and Stan scripts. Set the work directory to the Day 1 folder. Download the dataset for today and move them into the Day 1 folder: Bti_Larvicide_dataset.csv For Windows, the code for setting the work directory will be: setwd(&quot;C:/Users/AccountName/Desktop/CPD-course/Day 1&quot;) For MAC, the code for setting the work directory will be: setwd(&quot;/Users/AccountName/Desktop/CPD-course/Day 1&quot;) 2.1.5 Loading packages To start writing scripts for Bayesian analysis, we will need to load the rstan package. # Load the packages with library() library(&#39;rstan&#39;) Note that when you load rstan from cran, you will see some recommendations on using multiple cores for speeding the process. For the best experience, we highly recommend using this code: options(mc.cores = parallel::detectCores()) rstan_options(auto_write = TRUE) This tells RStudio to use multiple core for parallel processing whenever Stan is being implemented. Every time you want to use Stan - make sure to load parallel::detectCores() and rstan_options code. 2.2 About Stan Programming This section describes how to code up a basic Stan model. This section forms the foundation for later, and more complex models. 2.2.1 Basic building blocks I: Opening a Stan Script in RStudio Alright, let’s open a Stan file. You can do this by clicking and selecting File &gt; New File &gt; Stan File When you open a new Stan file, you will be greeted with an untitled script which contains the following bits of code: // // This Stan program defines a simple model, with a // vector of values &#39;y&#39; modeled as normally distributed // with mean &#39;mu&#39; and standard deviation &#39;sigma&#39;. // // Learn more about model development with Stan at: // // http://mc-stan.org/users/interfaces/rstan.html // https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started // // The input data is a vector &#39;y&#39; of length &#39;N&#39;. data { int&lt;lower=0&gt; N; vector[N] y; } // The parameters accepted by the model. Our model // accepts two parameters &#39;mu&#39; and &#39;sigma&#39;. parameters { real mu; real&lt;lower=0&gt; sigma; } // The model to be estimated. We model the output // &#39;y&#39; to be normally distributed with mean &#39;mu&#39; // and standard deviation &#39;sigma&#39;. model { y ~ normal(mu, sigma); } Do not worry about that - it is just some formalities put there by default by the developers behind rstan. You can delete everything you see in this script as we will build our own basic script from scratch. Once you have deleted the default information save the empty file in the Day 1 folder naming it Predicting_larvicide_concentrations.stan. Whenever you are saving Stan programme in RStudio always make sure to save it with a .stan ending. 2.2.2 Basic building blocks II: Structure of a Stan script A typical Stan script consist of the following 6 blocks: Data Transformed data Parameters Transformed parameters Model Generated quantities Out of these 6 blocks, the Data, Parameters and Model block must be specified in your Stan script. These are three compulsory blocks needed in any Stan script in order for a Bayesian model to work within the rstan environment. Let us define what these three important blocks are. FIRST: The data block allows the user to declare how the model reads the dataset from RStudio by specifying the sample size N or observations; the number of k parameters that needs to be estimated; the names or list of independent variables for the corresponding parameters (e.g., coefficients); the dependent variable (or outcome); as well as any constraints that needs to be applied to the dataset. A data block is specified accordingly in the script as: data { } It is within these curly brackets will specify these details of our dataset. They must be precise as it will have to correspond with the dataset that’s currently loaded in RStudio’s memory. SECOND: The parameters block allows the user to declare all the unknown quantities we are going to estimate. The parameters that go here are the ones we want to infer or predict. It define includes the name(s) of the parameters that correspond things like the mean, variance, sd, coefficients or anything that you are going to estimate. A parameters block is specified after the data block: data { } parameters { } THIRD: The model block allows the user to declare and specify the sampling statistical statements for the dependent variable (i.e., likelihood function) as well as any priors for the parameters we will estimate through the model. This block is where we code our probability distributions. A model block is specified after the parameters block: data { } parameters { } model { } Note that adding a double forward slashes // lets the user add a comment to the script. Let us add some comments into each of the blocks: // Add comments after double forward slashes data { // data block } parameters { // parameters block } model { // model block } Important Notes: Since, the other blocks are not compulsory - we will leave them out for now. But we will come back and explain what those remaining blocks are in Day 2 and 3. Now, save your Stan script. 2.2.3 Basic building blocks III: Data types and constraint declarations In Stan, all parameters and data must be defined as variables with a specific type. Note, this is quite a pain but going through this step allows rstan to perform really fast. There are four basic data types: int: for integers, used for specifying the sample size, and is applied to discrete variables real: for continuous, is applied to continuous variables (i.e., ratio or interval) array: for a containing several elements of data points that are real or int values vector: for a containing several elements of data points that are real or int values. It is similar to array syntax, but if you are going to perform algebraic manipulation of several elements with matrices - then it is best to use them as a vector. matrix: for containing a collection of several column vectors as a single n-by-m object. For constraints, we specify them on variables. For example, if we are dealing with a proportion p we will code it as real&lt;lower=0, upper=1&gt; p to tell Stan that p can be any value from 0 to 1, inclusive. Note that specifying constraints really helps speed Stan up so use them wherever you can. Lastly, you can create array of variables. For example, array[10] real&lt;lower=0 upper=1&gt; p tells Stan that p is an array of 10 real values for proportions. We can also create a matrix to represent a set of independent variables and so on. Please take you time with learning these codes, and experiment with them more to gain proficiency. Now that we have discussed these points - let us work with an actual demonstration to show data types and constraints work. 2.2.4 Basic building blocks IV: Developing our model PROBLEM: To combat mosquito populations - water bodies are treated with biological an agent called Bti dunks. These are ringed shaped objects that dissolve in standing water where mosquitoes breed. 120 samples from various points to ensure its working against mosquitoes and its not a cause for concern in terms of environmental contamination. What is the mean &amp; standard deviation, and uncertainty surrounding the levels of Bti larvicide concentrations present in standing water? If you open the following dataset Bti_Larvicide_dataset.csv: # open dataset Bti_data &lt;- read.csv(&quot;Bti_Larvicide_dataset.csv&quot;) # open data viewer View(Bti_data) You will notice that it contains 120 observations, and 2 columns: The first column standing_water_id is the unique ID number for each observation The second column Bti_mgl is the concentrations of Bti larvicides in various water bodies (in mg/L) Let us build our first model that predicts the mean and standard deviation as a posterior distribution. Let us extract the bits and pieces of information for the data block: Total sample size is 120. We will define that as N in the Stan as an integer int with non-zero values in the data block The column in the data frame for Bti_data (i.e., Bti_mgl) needs to be extracted. We will define that as an array of size N containing real values. We will name the variable as bti in Stan within the data block Now that we have the information, in RStudio, we will need to build this dataset into a list object using list() to connect it with the data block in the Stan script. Let us store the data in list object called stan_dataset: # create a list object using the list() function stan_dataset &lt;- list(N = 120, bti = Bti_data$Bti_mgl) # print data in console stan_dataset Now that the list object is prepared, we can start scripting in Stan. FIRST: We specify the total number of samples as N that its an integer int which cannot be negative a number &lt;lower = 0&gt; in the data block. Also, we also need to specify name of the bti variable in stan that its an array of size N consisting real numbers in the data block too. data { // define N (120) int&lt;lower = 0&gt; N; // create an array of size 120 to store BTI values array[N] real bti; } SECOND: For the parameters block, here we will need to specify the name of the parameters that we want to infer. Here, its \\(\\mu\\) which is the mean levels of Bti concentrations, as well as \\(\\sigma\\) which is the standard deviation. Note that, we need to tell Stan that these parameters are real numbers, as well as they are strictly positive values (i.e., use &lt;lower = 0&gt; to apply constraint) because we cannot have negative concentrations &amp; a negative value for standard deviation. data { // define N (120) int&lt;lower = 0&gt; N; // create an array of size 120 to store BTI values array[N] real bti; } parameters { // defined the mean as mu real&lt;lower = 0&gt; mu; // defined the SD as sigma real&lt;lower = 0&gt; sigma; } THIRD: For the model block, we need to state that both \\(\\mu\\) and \\(\\sigma\\) in the likelihood function - hence, bti variable will be sampled from the normal distribution. The model block will be: data { // define N (120) int&lt;lower = 0&gt; N; // create an array of size 120 to store BTI values array[N] real bti; } parameters { // defined the mean as mu real&lt;lower = 0&gt; mu; // defined the SD as sigma real&lt;lower = 0&gt; sigma; } model { // likelihood function, without any prior distribution specified bti ~ normal(mu, sigma); } COMPLIMENTS: Well done, we have built our first Bayesian model in Stan. Let us now save this script. What we need to do next is compile this script in RStudio to get our results. 2.2.5 Basic building blocks V: Compiling Stan code in RStudio You can use the stan() function to call the stan script from RStudio in order to obtain the posterior estimates. The results will be stored in stan object called bti_prediction: # the directory needs to be set to where you save the dataset and Stan script bti_prediction &lt;- stan(&quot;Predicting_larvicide_concentrations.stan&quot;, data=stan_dataset, iter=3000, chains=3, verbose = FALSE) Some notes on the above code’s arguments: data= specify the dataset stored as a list object. iter= we are asking the stan() to perform 3,000 iterations on each chain to generate the posterior samples. The algorithm behind these iterations can be MCMC, NUTS or HMC algorithm (NUTS No-U-turn sampler is the default) chains= we are asking the stan() function to perform 3 chains using 3 cores in our computer The resulting output can be printed with the function print(). Here, we are going to print the mean, standard error in mean, SD and the IQR ranges with 95% limits (i.e., 2.5% and 97.5%): print(bti_prediction, probs=c(0.025, 0.25, 0.5, 0.75, 0.975)) We obtain this summary table: Inference for Stan model: anon_model. 3 chains, each with iter=3000; warmup=1500; thin=1; post-warmup draws per chain=1500, total post-warmup draws=4500. mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat mu 25.27 0.01 0.45 24.39 24.96 25.26 25.57 26.18 3624 1 sigma 5.01 0.01 0.34 4.40 4.77 4.99 5.22 5.72 3811 1 lp__ -247.60 0.02 1.03 -250.35 -247.97 -247.28 -246.87 -246.61 2164 1 Samples were drawn using NUTS(diag_e) at Mon Jun 9 01:10:46 2025. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). What does it all mean? The top part states that 3 chains were run for 3000 iterations. However, the first 1500 samples generated from each chain were discarded as warm-up, meaning that only 1500 samples from each chain were kept, resulting 4500 (1500x3) total post-warm-up sample draws. The output shows the summary statistics for our \\(\\mu\\) and \\(\\sigma\\). The lp__ is the log-probability - which is used to quantify how well the model is for the data but, in my opinion, its not a useful estimate. Instead, use the effective sample size n_eff and Rhat. If the Rhat is less than 1.05 for all parameters - it means that the estimation of our parameters are fine. Anything above 1.05 it means that the results are not reliable. 2.2.6 Basic building blocks VI: Extract posterior samples &amp; interpretation At this point, let us extract the posterior samples for the mean and standard deviation, and graph them to understand it posterior distribution. We use the extract() function from the rstan package, and graph them: # extracting the samples (it should be 4500) extracted_samples &lt;- rstan::extract(bti_prediction) The object extracted_samples should contain the 4,500 sample results each for the Bti (mg/L) mean and its corresponding SD. From the extraction, suppose if you wanted to compute the posterior mean, with the 0.025 and 0.975 quantiles (these limits are referred to as 95% credibility limits (95% CrI)). This can be done using both the mean() and quantile() function. Here is how we compute them: # posterior mean for mu mean(extracted_samples$mu) # calculate 95% Credibility limits (95% CrI) for mu quantile(extracted_samples$mu, probs = c(0.025, 0.975)) We can also examine the posterior samples as a distribution as a visual through a density plot: # generate probability density plot of the posterior samples plot(density(extracted_samples$mu), main = &quot;Posterior samples&quot;, xlab = &quot;Posterior Means for Bti concentrations (mg/L)&quot;, ylab = &quot;Posterior Probability Density (Plausibility)&quot;) # Add vertical dashed line at the mean abline(v = 25.24285, lty = &quot;dashed&quot;, col = &quot;darkgrey&quot;, lwd = 2) # add vertical line for the lower 95% CrI value abline(v = 24.34943, lty = &quot;dashed&quot;, col = &quot;darkgrey&quot;, lwd = 2) # add vertical line for the upper 95% CrI value abline(v = 26.12595, lty = &quot;dashed&quot;, col = &quot;darkgrey&quot;, lwd = 2) We can also report the results for the standard deviation as well (i.e., its posterior mean, 95% CrI and posterior distribution): # posterior mean for sigma mean(extracted_samples$sigma) # calculate 95% Credibility limits (95% CrI) for sigma quantile(extracted_samples$sigma, probs = c(0.025, 0.975)) # generate probability density plot of the posterior samples for sigma plot(density(extracted_samples$sigma), main = &quot;Posterior samples&quot;, xlab = &quot;Posterior Means for Standard Deviation&quot;, ylab = &quot;Posterior Probability Density (Plausibility)&quot;) # Add vertical dashed line at the mean abline(v = 4.998751, lty = &quot;dashed&quot;, col = &quot;darkgrey&quot;, lwd = 2) # add vertical line for the lower 95% CrI value abline(v = 4.411431, lty = &quot;dashed&quot;, col = &quot;darkgrey&quot;, lwd = 2) # add vertical line for the upper 95% CrI value abline(v = 5.666914, lty = &quot;dashed&quot;, col = &quot;darkgrey&quot;, lwd = 2) Interpretation: The estimated mean (\\(\\mu\\)) of Bti concentrations from our sample posterior distribution was 25.24 mg/L (i.e., most likely or plausible value) with 95% credibility limits of 24.39 and 26.12. Formally writing as \\(\\mu\\) = 25.24 mg/L (95% CrI: 24.39-26.12). In terms of the spread, the estimated standard deviation (\\(\\sigma\\)) is 4.99 mg/L (95% CrI: 4.411 to 5.666) 2.3 Tasks 2.3.1 Task 1 - Simulating and Estimating Body Mass Index (BMI) Try this first problem in Stan: You are tasked to simulate a sample of 1000 BMI values from a Normal distribution with mean 23 and SD 8.3. Then, using Stan, fit a Bayesian model to estimate the unknown population mean BMI and its 95% credible interval. Assume both the population mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)) are unknown. Hints: In the R script, use the function rnorm() to generate your sample of 1000 BMI points In the R script, create a list() with N and bmi (your simulated data) In the Stan script for the following: Define the data block in accordance with the list object. Use a parameters block to define mu and sigma as real values (constrain sigma to be positive). Use a model block to write the likelihood: bmi ~ normal(mu, sigma); Report the posterior mean of BMI and its 95% credible interval from the model output. Generate its posterior density plot. 2.3.2 Task 2 - Low-level arsenic poisoning in Cornwall, UK Try this second problem in Stan: Suppose a random sample of 50 villagers from a rural community in Cornwall, UK, were surveyed for signs of chronic arsenic exposure. Each person was classified as either Diseased or Healthy. Out of the 50 surveyed, 19 were found to be diseased. Use a Bayesian model to estimate the true prevalence of arsenic poisoning in the wider population. Hints: In the R script, create a list() with N and Diseased. In the Stan script for the following: Define the data block in accordance with the list object. Use a parameters block to define prevalence as a parameter (constrain prevalence between 0 and 1). Use a model block to write the likelihood: Diseased ~ binomial(N, prevalence); Report the posterior mean of the prevalence of metallic poisoning in Cornwall with its 95% credible interval from the model output. Generate its posterior density plot. Note: Solutions for task 1 and 2 will be made available later today "],["introduction-to-bayesian-inference.html", "3 Introduction to Bayesian Inference 3.1 Introduction 3.2 Input: Data, Process Model and Assumptions for Priors 3.3 Compiling Stan code in RStudio 3.4 Tasks", " 3 Introduction to Bayesian Inference 3.1 Introduction 3.1.1 Lectures (Length: 1:15:22) [Watch on YouTube] 3.1.2 Learning outcomes Today’s session aims to introduce you to the basic Stan programming etiquette for Bayesian analysis in RStudio using Stan as an Interface, and producing output and interpreting it’s results. By the end of this session, you should be able to perform the following: Getting acquainted with Stan and learning the basic programming etiquette of Stan - this will include simulating and estimation of parameters. Writing you process model and prior distribution for the parameters. Know how to compile the results into RStudio for use, interpretation and visualisation. After the demonstration session is delivered in the first 1-hour of the practical - you can use the computer practical session to try the tutorials yourself by following the instructions as well as trying out the tasks. 3.1.3 Demonstration (Length: 1:22:54) [Watch on YouTube] 3.1.4 Setting up the work directory Go the existing folder CPD-course and create a sub folder called “Day 2” within the CPD-course folder. Here, we will store all our R and Stan scripts. Set the work directory to the Day 2 folder. For Windows, the code for setting the work directory will be: setwd(&quot;C:/Users/AccountName/Desktop/CPD-course/Day 2&quot;) For MAC, the code for setting the work directory will be: setwd(&quot;/Users/AccountName/Desktop/CPD-course/Day 2&quot;) 3.1.5 Loading packages We will need to load the rstan package and perform the set-up for Stan by setting the number of cores accordingly. # Load the packages with library() library(&#39;rstan&#39;) # set up options(mc.cores = parallel::detectCores()) rstan_options(auto_write = TRUE) Remember, this tells RStudio to use multiple core for parallel processing whenever Stan is being implemented. Every time you want to use Stan - make sure to load parallel::detectCores() and rstan_options code. 3.2 Input: Data, Process Model and Assumptions for Priors This section describes how to code up a basic Stan model from scratch. Here, we will use an example of a mechanistic model for predicting incident disease outcome. This section forms the foundation for later, and more complex models. PROBLEM: Bayesian estimation of Epidemic Growth for Aedes-borne Infections -During the early days of heavy rainfall - there is a huge influx of mosquito populations in Recife which causes an outbreak Dengue virus. Public health officials collected daily case counts over the course of 15 days during this period where rainfalls are pronounced. The process model for predicting dengue is: \\(D(t) = D_0 \\exp(rt)\\) What is the initial value of dengue cases \\(D_0\\) as an incidence? Note that \\(D(t)\\) is the observed number of cases at time (\\(t\\)) What is the estimated epidemic growth rate \\(r\\)? Enter the following dataset: # Simulated data day &lt;- 0:14 observed_cases &lt;- c(12, 9, 19, 30, 27, 45, 67, 71, 103, 119, 161, 213, 288, 340, 431) Let us extract the bits and pieces of information for the data block: Total sample size is 15 which corresponds to the number of time points starting from 0 to 14. We will define that as N in the Stan as an integer int with non-zero values in the data block We are going to read each day point separately from 0 to 14. We will define that as t in the Stan as an array of size N to hold the collection of values. These are non-zero values in the data block The array for observed_cases needs to be extracted. We will define that as an array of size N containing int values. We will name the variable as y in Stan within the data block Now that we have the information, in RStudio, we will need to build this dataset into a list object using list() to connect it with the data block in the Stan script. Let us store the data in list object called stan_dataset: # create a list object using the list() function # Data list for Stan stan_dataset &lt;- list(N = length(day), t = as.vector(day), y = as.integer(observed_cases) ) # print data in console stan_dataset Now that the list object is prepared, we can start scripting in Stan. FIRST: We specify the total number of samples as N that its an integer int which cannot be negative a number &lt;lower = 0&gt; in the data block. Also, we also need to specify name of the t and y variables in stan that its an array of size N consisting of positive int numbers in the data block too. data { int&lt;lower = 0&gt; N; array[N] int&lt;lower = 0&gt; t; array[N] int&lt;lower = 0&gt; y; } SECOND: For the parameters block, here we will need to specify the name of the parameters that we want to infer. Here, its \\(D_0\\) which is the mean initial (or baseline) cases of Dengue, as well as \\(r\\) which is the mean growth rate for the epidemic. Note that, we need to tell Stan that these parameters real numbers, where the former is strictly positive values (i.e., use &lt;lower = 0&gt; to apply constraint), and \\(r\\) can be either negative or positive. data { int&lt;lower = 0&gt; N; array[N] int&lt;lower = 0&gt; t; array[N] int&lt;lower = 0&gt; y; } parameters { real&lt;lower = 0&gt; D0; real r; } THIRD: For the model block, we need to create mechanistic model for the \\(D(t)\\). Howeever, we are going to state some priors as follows: Prior assumptions: \\(D_0\\) could be anything, but it is mostly likely 10, and probably between 1 to 20, anything higher is less likely. This corresponds to a gamma distribution i.e., \\(D_0\\) ~ gamma(2, 0.1), which is a weakly informative prior \\(r\\) could be anything, it is hard to say – therefore I will be cautious and assume that it is negligible (0); however, the growth rate could be an increasing, or a decreasing value, and so I will assume a SD of 1. This corresponds to a normal distribution i.e., \\(r\\) ~ normal(0, 1), hence, its Non-informative prior Likelihood function for process model - The best suited likelihood function for the process model is a Poisson. Why? Because the observed data i.e., observed_cases is discrete and not continuous. Here is what the Stan code will look like: data { int&lt;lower = 0&gt; N; array[N] int&lt;lower = 0&gt; t; array[N] int&lt;lower = 0&gt; y; } parameters { real&lt;lower = 0&gt; D0; real r; } model { // priors D0 ~ gamma(2, 0.1); r ~ normal(0, 1); for (n in 1:N) { real lambda = D0 * exp(r * t[n]); y[n] ~ poisson(lambda); } } Lastly, we will add one more block called the generated quantities block to make predictions about the incidence by generating simulated data that resembles the original. data { int&lt;lower = 0&gt; N; array[N] int&lt;lower = 0&gt; t; array[N] int&lt;lower = 0&gt; y; } parameters { real&lt;lower = 0&gt; D0; real r; } model { D0 ~ gamma(2, 0.1); r ~ normal(0, 1); for (n in 1:N) { real lambda = D0 * exp(r * t[n]); y[n] ~ poisson(lambda); } } generated quantities { vector[N] y_pred; for (n in 1:N) { y_pred[n] = poisson_rng(D0 * exp(r * t[n])); } } COMPLIMENTS: Well done, we have built our second Bayesian model in Stan. Let us now save this script. What we need to do next is compile this script in RStudio to get our results. 3.3 Compiling Stan code in RStudio You can use the stan() function to call the stan script from RStudio in order to obtain the posterior estimates. The results will be stored in stan object called bti_prediction: fit &lt;- stan( file = &quot;Incidence_rates.stan&quot;, data = stan_dataset, iter = 3000, chains = 3, verbose = FALSE ) Recall that the above code’s arguments are as follows: data= specify the dataset stored as a list object. iter= we are asking the stan() to perform 3,000 iterations on each chain to generate the posterior samples. The algorithm behind these iterations can be MCMC, NUTS or HMC algorithm (NUTS No-U-turn sampler is the default) chains= we are asking the stan() function to perform 3 chains using 3 cores in our computer The resulting output can be printed with the function print(). Here, we are going to print the mean, standard error in mean, SD and the IQR ranges with 95% limits (i.e., 2.5% and 97.5%): print(fit, pars = c(&quot;D0&quot;, &quot;r&quot;), probs = c(0.025, 0.5, 0.975)) We obtain this summary table: Inference for Stan model: anon_model. 3 chains, each with iter=3000; warmup=1500; thin=1; post-warmup draws per chain=1500, total post-warmup draws=4500. mean se_mean sd 2.5% 50% 97.5% n_eff Rhat D0 12.31 0.04 1.06 10.31 12.26 14.51 771 1 r 0.26 0.00 0.01 0.24 0.26 0.27 784 1 Samples were drawn using NUTS(diag_e) at Tue Jun 10 08:37:14 2025. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). What does it all mean? The top part states that 3 chains were run for 3000 iterations. However, the first 1500 samples generated from each chain were discarded as warm-up, meaning that only 1500 samples from each chain were kept, resulting 4500 (1500x3) total post-warm-up sample draws. The output shows the summary statistics for our \\(\\mu\\) and \\(\\sigma\\). The lp__ is the log-probability - which is used to quantify how well the model is for the data but, in my opinion, its not a useful estimate. Instead, use the effective sample size n_eff and Rhat. If the Rhat is less than 1.05 for all parameters - it means that the estimation of our parameters are fine. Anything above 1.05 it means that the results are not reliable. Interpretation: The estimated growth rate for incident Dengue 0.26 (95% CrI: 0.24 to 0.27). The estimated mean value for \\(D_0\\) was 12.31 (95% CrI: 10.31 to 14.51) At this point, let us extract the posterior samples for the mean and standard deviation, and graph them to understand it posterior distribution. We can also see if the predictions are similar to original data. # extracting the samples (it should be 4500) # Extract and plot posterior predictions posterior &lt;- extract(fit) y_pred_mean &lt;- apply(posterior$y_pred, 2, mean) Here is the plot: # plot observed vs predictions plot(day, observed_cases, pch = 16, col = &quot;red&quot;, xlab = &quot;Day&quot;, ylab = &quot;Cases&quot;, main = &quot;Observed vs. Posterior Predicted Cases&quot;) # add cosmetics to plot lines(day, y_pred_mean, col = &quot;blue&quot;, lwd = 2) legend(&quot;topleft&quot;, legend = c(&quot;Observed&quot;, &quot;Predicted Mean&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), pch = c(16, NA), lty = c(NA, 1)) You may be interested in seeing the simulated samples that was drawn from the joint distribution. You can pull this information and store as a data frame: posterior_df &lt;- data.frame( D0 = posterior$D0, r = posterior$r, predictions = posterior$y_pred ) 3.4 Tasks 3.4.1 Task 1 - Aedes-borne infestation in Recife 05/2023 Try this second problem in Stan: Suppose a random sample of 976 households from Recife, Brazil, were surveyed Aedes-borne infestation. From the total, 428 were affected by mosquito infestation. It is common knowledge that prevalence of infestation is rough about 20-25%. Use a Bayesian model to estimate the true prevalence of infestation in Recife. Hints: In the R script, create a list() with N and Infested. In the Stan script for the following: Define the data block in accordance with the list object. Use a parameters block to define prevalence as a parameter (constrain prevalence between 0 and 1). Use a model block to write the likelihood: Infested ~ binomial(N, prevalence); In the model block, use it to specify the appropriate prior distribution for the prevalence parameter. Report the posterior mean of the prevalence of infestation in Recife with its associated 95% credibility interval from the model output. Generate its posterior density plot. "],["bayesian-generalised-linear-models-glms.html", "4 Bayesian Generalised Linear Models (GLMs) 4.1 Introduction 4.2 Poisson Regression Modelling 4.3 Tasks", " 4 Bayesian Generalised Linear Models (GLMs) 4.1 Introduction 4.1.1 Lectures (Length: 1:04:16) [Watch on YouTube] 4.1.2 Learning outcomes Today’s session aims to formally introduce you to Stan programming for Bayesian regression models. By the end of this session, you should be able to perform the following: Select the appropriate likelihood function specification for the Bayesian regression model i.e., normal, binomial or Poisson to model either continuous, binary, or count outcomes respectively; How to fully develop Stan code for such regression models with the appropriate prior (i.e., uninformative, weak or informative) specification for various parameters; How to interpret the various types of coefficients including Odds Ratios (OR) and Risk Ratios (RR); Computing exceedance probabilities; Model validation You can follow the live walkthrough demonstration, and then use the practical sessions to try the practical tutorials yourself by following the instructions and trying out the tasks. 4.1.3 Demonstration [Part I] (Length: 1:59:36) [Watch on YouTube] 4.1.4 Demonstration [Part II] (Length: 1:13:38) [Watch on YouTube] 4.1.5 Datasets &amp; setting up the work directory Go to your folder CPD-course and create a sub folder called “Day 3”. Here, we will store all our R &amp; Stan scripts as well as the dataset for this session. Set your work directory to Day 3’s folder. For Windows, the code for setting the work directory will be: setwd(&quot;C:/Users/AccountName/Desktop/CPD-course/Day 3&quot;) For MAC, the code for setting the work directory will be: setwd(&quot;/Users/AccountName/Desktop/CPD-course/Day 3&quot;) The dataset for this practical: Street Burglary Data in Nigeria.csv The datasets for the task at the end of the practical: London LSOA 2015 data.csv Obesity and Fastfoods in MSOAs data.csv Let us start with the crime data titled: Street Burglary Data in Nigeria.csv Context about the dataset: Conventional analyses of crime, based on European research models, are often poorly suited to assessing the specific dimensions of criminality in Africa or elsewhere in the Global South. The data used in today’s practical is an anonymised resampled excerpt from the Development Frontiers in Crime, Livelihoods and Urban Poverty in Nigeria (FCLP) project that aimed to provide an alternative framework for understanding the specific drivers of criminality in a West African urban context. This research project used a mixed-methods approach for combining statistical modeling, geovisualisation and ethnography, and attempted to situate insecurity and crime against a broader backdrop of rapid urban growth, seasonal migration, youth unemployment and informality. The study typically provided researchers both in Nigeria and internationally a richer and more nuanced evidence base on the particular dynamics of crime from an African perspective resulting a number of publications: [1], [2] and [3]. We will reproduce some of the analysis using a Bayesian regression model. 4.1.6 Loading and installing packages We will need to load the following packages: # Load the packages with library() library(&#39;rstan&#39;) library(&quot;loo&quot;) library(&quot;MASS&quot;) For the best experience with rstan, we highly recommend using this code to set the cores: options(mc.cores = parallel::detectCores()) rstan_options(auto_write = TRUE) 4.2 Poisson Regression Modelling We are going to fit a Poisson-type model on an outcome that contains discrete counts of households reporting to have been victims of burglary. These are counts of burglary events aggregated to street segments. Let us load the data into RStudio and call the object burglaryDataset. # Set your own directory using setwd() function # Load data into RStudio using read.csv(). The spreadsheet is stored in the object called &#39;burglaryData&#39; burglaryDataset &lt;- read.csv(&quot;Street Burglary Data in Nigeria.csv&quot;) names(burglaryDataset) 4.2.1 Selecting the appropriate Poisson model There are three different types of Poisson models: Standard Poisson regression Negative Binomial Poisson regression Zero-Inflated Poisson regression The implementation of one of these models are highly dependent on how the frequency distribution of the count response variable are displayed. If it resembles a normal curve - then use the standard Poisson version. Otherwise, use the Negative Binomial Poisson regression if there is any evidence of over-dispersion. When there’s an inflation of zero counts in the dataset, you will have to use the Zero-Inflated model to account for this problem. Let’s check the frequency distribution of the outcome variable Burglary which corresponds to the number of reported instances a property on a street was burgled. You can simply use a histogram to examine its distribution: # see lowest count min(burglaryDataset$burglary) # see highest count max(burglaryDataset$burglary) # visual distribution hist(burglaryDataset$burglary, breaks=20, xlim = c(0, 25), ylim = c(0, 600), xlab = &quot;Report number of burglaries on a street segment&quot;, ylab = &quot;Frequency&quot;, main = &quot;Distribution of Burglary counts&quot;) The plot show evidence of over-dispersion. It indicates that a high number of streets in this city have less frequency of burglaries, while a few number of streets have reported excess number of burglaries. Here, we consider using a Negative Binomial Poisson regression model over the standard and zero-inflated versions (i.e., scenario 1 and 3). Now, that we know the model type, let us estimate the over-dispersion parameter using the glm.nb() function. # Fit negative binomial regression null model nb_model &lt;- glm.nb(burglary ~ 1, data = burglaryDataset) # Extract theta theta &lt;- nb_model$theta theta [1] 0.3161472 The estimated over-dispersion parameter is 0.3161472, which is small, this suggests that residential burglaries exhibits substantial over-dispersion. We will use this value in our Bayesian model when we code it in Stan. Acknowledgements: Thanks to Fernando Rodriguez for pointing out that it is improper practice to fix the dispersion parameter in the model, and for advising that it is best practice to allow the model to estimate it instead. That part has been amended accordingly. Further thanks to Jacob Cohen for noting that the current approach to estimating the dispersion parameter should serve primarily as guidance only to help us understand the extent of dispersion in our data and inform the choice of a prior distribution accordingly. 4.2.2 Data preparation and set-up for Bayesian analysis in Stan Let begin with a model that only contains independent variables that are continuous measures i.e., distance and connectivity. We can prepare the dataset into list() object: stan_dataset_model1 &lt;- list(N = nrow(burglaryDataset), burg = burglaryDataset$burglary, dist = burglaryDataset$distance, conn = burglaryDataset$connectivity, offset = log(burglaryDataset$totalhouses) ) Important Notes: The list object stan_dataset_model1 from RStudio is passed into the Stan. N = nrow(burglaryDataset) we are extracting the number of observations present in the dataset. Note that this a smart way instead of hard coding the number. Note that here, N is 743, meaning there 743 street segments. burg = burglaryDataset$burglary: Here, we defined the outcome variable (i.e., counts of burglaries) as burg. dist = burglaryDataset$distance: Independent variable for distance. conn = burglaryDataset$connectivity: Independent variable for connectivity. off_set = log(burglaryDataset$totalhouses): Here, the off_set is calculated from taking the log-transform of the totalhouses, which is the denominators used for expressing the residential burglaries as a crime rate per capita. Let’s create our Stan script for running a Negative Binomial Poisson regression within a Bayesian framework. 4.2.3 Creating a script to run a Negative Binomial Poisson regression in Stan A typical Stan program for a regression consist of the following 5 blocks: Data Parameters Model Generated quantities The Data, Parameters and Model block must be specified for the regression to work. But there will be additional block that we will need to transform the resultant parameters (i.e., coefficients) into relative risk (RR) inside the Generated quantities block. Let’s start with the data block: FIRST STEP: We specify the total number of observations N as an integer, as well as the information we defined in our list object stan_dataset_model1 to be passed to Stan. This information is specified in the data block: data { int&lt;lower=0&gt; N; // declare the overall number of data points to be passed into model array[N] int&lt;lower=0&gt; burg; // define as an array and specify it as an integer for counts vector[N] dist; // continuous variable vector[N] conn; // continuous variable vector[N] off_set; // offset variable for the denominators (total households on a stree segment) } SECOND STEP: For the parameters block, here we will need to specify the name of the regression intercept alpha, which is baseline risk of residential burglaries, and the two coefficients i.e., beta[1] and beta[2] for our two independent variables dist and conn respectively. We will also create phi, a dispersion parameter, to mimic the behaviour of that shown in the histogram of Scenario 2 and the derived estimate of \\(\\theta\\) = 0.3161472 (see section 4.2.1) data { int&lt;lower=0&gt; N; // declare the overall number of data points to be passed into model array[N] int&lt;lower=0&gt; burg; // define as an array and specify it as an integer for counts vector[N] dist; // continuous variable vector[N] conn; // continuous variable vector[N] off_set; // offset variable for the denominators (total households on a stree segment) } parameters { real alpha; real&lt;lower=0&gt; phi; vector[2] beta; } THIRD STEP: We build our likelihood function and specify the priors for each parameter under the model block. For all parameters - the priors have been centred around 0, meaning that broadly, these variables have no effect on residential burglaries, and if any, these effects may range from ±1SD (so in risk terms 0.36 to 2.71). The regression model is neg_binomial_2_log(formula, Overdispersion): data { int&lt;lower=0&gt; N; // declare the overall number of data points to be passed into model array[N] int&lt;lower=0&gt; burg; // define as an array and specify it as an integer for counts vector[N] dist; // continuous variable vector[N] conn; // continuous variable vector[N] off_set; // offset variable for the denominators (total households on a stree segment) } parameters { real alpha; real&lt;lower=0&gt; phi; vector[2] beta; } model { // prior specification for our parameters alpha ~ normal(0, 1); beta[1] ~ normal(0, 1); beta[2] ~ normal(0, 1); phi ~ beta(2, 5) // likelihood function i.e., statistical model for (i in 1:N) { burg[i] ~ neg_binomial_2_log(alpha + beta[1]*dist[i] + beta[2]*conn[i] + offset[i], phi); } } LAST STEP: We instruct Stan to generated quantitites to calculate the relative risk ratio (RRs) by converting the estimated coefficients by using exp(). We ask it to calculate the log likelihood for model validation: data { int&lt;lower=0&gt; N; // declare the overall number of data points to be passed into model array[N] int&lt;lower=0&gt; burg; // define as an array and specify it as an integer for counts vector[N] dist; // continuous variable vector[N] conn; // continuous variable vector[N] off_set; // offset variable for the denominators (total households on a stree segment) } parameters { real alpha; real&lt;lower=0&gt; phi; vector[2] beta; } model { // prior specification for our parameters alpha ~ normal(0, 1); beta[1] ~ normal(0, 1); beta[2] ~ normal(0, 1); phi ~ beta(2, 5) // likelihood function i.e., statistical model for (i in 1:N) { burg[i] ~ neg_binomial_2_log(alpha + beta[1]*dist[i] + beta[2]*conn[i] + off_set[i], phi); } } generated quantities { // report crime risk ratios real baselineCrimeRR; vector[2] CrimeRR; baselineCrimeRR = exp(alpha); CrimeRR = exp(beta); // model validation and comparison vector[N] log_lik; for (i in 1:N) { log_lik[i] = neg_binomial_2_log_lpmf(burg[i] | alpha + beta[1]*dist[i] + beta[2]*conn[i] + off_set[i], phi); } } Save the stan script as Cont_Model_1.stan. 4.2.4 Compiling our Stan code in RStudio and Results Now, let us turn our attention to RStudio. Using the stan() to compile and obtain the posterior estimation of the overall risk and crime risk ratios (CRR) for the each independent variable: # the directory needs to be set to where you saved the dataset and Stan script crr.negbin.model1 = stan(&quot;Cont_Model_1.stan&quot;, data=stan_dataset_model1, iter=3000, chains=6, verbose = FALSE) We can print the results accordingly: # reports all results print(crr.negbin.model1, pars = c(&quot;alpha&quot;, &quot;beta&quot;, &quot;baselineCrimeRR&quot;, &quot;CrimeRR&quot;), probs = c(0.025, 0.975)) Output summary table Inference for Stan model: anon_model. 6 chains, each with iter=3000; warmup=1500; thin=1; post-warmup draws per chain=1500, total post-warmup draws=9000. mean se_mean sd 2.5% 97.5% n_eff Rhat alpha -2.16 0 0.13 -2.42 -1.91 3282 1 beta[1] 0.00 0 0.00 0.00 0.00 4237 1 beta[2] 0.06 0 0.02 0.02 0.11 3216 1 baselineCrimeRR 0.12 0 0.02 0.09 0.15 3225 1 CrimeRR[1] 1.00 0 0.00 1.00 1.00 4236 1 CrimeRR[2] 1.07 0 0.02 1.02 1.11 3215 1 Samples were drawn using NUTS(diag_e) at Fri Jan 24 02:30:42 2025. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). Here is the messing part - interpretation. Before anything, note that alpha, beta[1] and beta[2] corresponds to the intercept, and coefficients for distance and connectivity, respectively. These are on the log-scale! The risk ratios from alpha, beta[1] and beta[2] which, in turn, corresponds to the intercept, and coefficients for distance and connectivity, respectively, are the baselineCrimeRR, CrimeRR[1] and CrimeRR[2]. For instance, the significant result is for connectivity, which shows for every unit increase in the number to street connections on a segment in the network, the risk of residential burglaries increase by 1.07 (7%), which is significant based on its 95% Credibility Interval (95 CrI: 1.02-1.11). We can compute exceedance probabilities i.e., that such risk concerning connectivity are greater than 1 (meaning there’s an excess risk): # Here, we can extract the simulated sample for CrimeRR[2] conn_draws &lt;- extract(crr.negbin.model1, &#39;CrimeRR[2]&#39;)[[1]] mean(conn_draws &gt; 1.00) [1] 0.9977778 This indicates a strong chance (99.7%) that streets with more connections to other street segments in the network will certainly increase the risk of residential burglaries. Lastly, we will assess the validity of our model using a test called Leave-One-Out Cross-Validation. The estimate from this test Expected Log Predictive Density (ELPD) quantifies how well a model performs and so higher values of ELPD indicates better predictive performance. While, it can be interpreted on its own, its primary value lies in model comparison rather than as an absolute measure. # extracting ELPD leave-one-out results from model with continuous variables only log_lik_with_cont &lt;- extract_log_lik(crr.negbin.model1, merge_chains = FALSE) r_eff_with_cont &lt;- relative_eff(log_lik_with_cont) loo_cont &lt;- loo(log_lik_with_cont, r_eff_with_cont, cores = 6) print(loo_cont) Computed from 9000 by 743 log-likelihood matrix. Estimate SE elpd_loo -1022.1 37.7 p_loo 2.6 0.4 looic 2044.3 75.3 ------ MCSE of elpd_loo is 0.0. MCSE and ESS estimates assume independent draws (r_eff=1). All Pareto k estimates are good (k &lt; 0.7). See help(&#39;pareto-k-diagnostic&#39;) for details. We take note that the ELPD is -1022.1 4.2.5 Model with categorical variables The purpose of this section is to show: How to include categorical variables into the model Perform model comparisons Let’s include the following categorical variables: integq and choiceq into the regression. Each variable has four categories, we want to omit the first category as it represents the lowest exposure group, and use it a reference for the higher categories when we estimate their risk. # See table for categorical variables table(burglaryDataset$choiceq) table(burglaryDataset$integq) # Create dummy variables for choice burglaryDataset$choicecat2 &lt;- ifelse(burglaryDataset$choiceq == 2, 1, 0) burglaryDataset$choicecat3 &lt;- ifelse(burglaryDataset$choiceq == 3, 1, 0) burglaryDataset$choicecat4 &lt;- ifelse(burglaryDataset$choiceq == 4, 1, 0) # Create dummy variables for integration burglaryDataset$integ2 &lt;- ifelse(burglaryDataset$integq == 2, 1, 0) burglaryDataset$integ3 &lt;- ifelse(burglaryDataset$integq == 3, 1, 0) burglaryDataset$integ4 &lt;- ifelse(burglaryDataset$integq == 4, 1, 0) Let us expand the model to include independent variables that are both continuous and categorical measures i.e., distance and connectivity, choice and integration, and the new information into a list() object: stan_dataset_model2 &lt;- list(N = nrow(burglaryDataset), burg = burglaryDataset$burglary, dist = burglaryDataset$distance, conn = burglaryDataset$connectivity, chcat2 = burglaryDataset$choicecat2, chcat3 = burglaryDataset$choicecat3, chcat4 = burglaryDataset$choicecat4, intcat2 = burglaryDataset$integ2, intcat3 = burglaryDataset$integ3, intcat4 = burglaryDataset$integ4, off_set = log(burglaryDataset$totalhouses) ) The amended Stan code will look something as follows: data { int&lt;lower=0&gt; N; // declare the overall number of data points to be passed into model array[N] int&lt;lower=0&gt; burg; // define as an array and specify it as an integer for counts vector[N] dist; // continuous variable vector[N] conn; // continuous variable vector[N] chcat2; vector[N] chcat3; vector[N] chcat4; vector[N] intcat2; vector[N] intcat3; vector[N] intcat4; vector[N] off_set; // offset variable for the denominators (total households on a stree segment) } parameters { real alpha; real&lt;lower=0&gt; phi; vector[2] beta; vector[3] chcq; vector[3] intq; } model { // prior specification for our parameters alpha ~ normal(0, 1); beta ~ normal(0, 1); chcq ~ normal(0, 1); intq ~ normal(0, 1); phi ~ beta(2, 5) // likelihood function i.e., statistical model for (i in 1:N) { burg[i] ~ neg_binomial_2_log(alpha + beta[1]*dist[i] + beta[2]*conn[i] + chcq[1]*chcat2[i] + chcq[2]*chcat3[i] + chcq[3]*chcat4[i] + intq[1]*intcat2[i] + intq[2]*intcat3[i] + intq[3]*intcat4[i] + off_set[i], phi); } } generated quantities { // report crime risk ratios real baselineCrimeRR; vector[2] betaRR; vector[3] chcqRR; vector[3] intqRR; baselineCrimeRR = exp(alpha); betaRR = exp(beta); chcqRR = exp(chcq); intqRR = exp(intq); // model validation and comparison vector[N] log_lik; for (i in 1:N) { log_lik[i] = neg_binomial_2_log_lpmf(burg[i] | alpha + beta[1]*dist[i] + beta[2]*conn[i] + chcq[1]*chcat2[i] + chcq[2]*chcat3[i] + chcq[3]*chcat4[i] + intq[1]*intcat2[i] + intq[2]*intcat3[i] + intq[3]*intcat4[i] + off_set[i], phi); } } Please save the Stan script as Cat_Model_2.stan. We can use the stan() to compile the updated code to obtain the posterior estimation accordingly: # the directory needs to be set to where you saved the dataset and Stan script crr.negbin.model2 = stan(&quot;Cat_Model_2.stan&quot;, data=stan_dataset_model2, iter=3000, chains=6, verbose = FALSE) We can print the results accordingly: # reports all results print(crr.negbin.model2, pars = c(&quot;baselineCrimeRR&quot;, &quot;betaRR&quot;, &quot;chcqRR&quot;, &quot;intqRR&quot;), probs = c(0.025, 0.975)) Output summary table Inference for Stan model: anon_model. 6 chains, each with iter=3000; warmup=1500; thin=1; post-warmup draws per chain=1500, total post-warmup draws=9000. mean se_mean sd 2.5% 97.5% n_eff Rhat baselineCrimeRR 0.13 0 0.03 0.09 0.20 4190 1 betaRR[1] 1.00 0 0.00 1.00 1.00 11128 1 betaRR[2] 1.08 0 0.03 1.03 1.14 5524 1 chcqRR[1] 0.95 0 0.21 0.60 1.44 4692 1 chcqRR[2] 0.94 0 0.23 0.57 1.46 4593 1 chcqRR[3] 0.86 0 0.29 0.44 1.55 4266 1 intqRR[1] 0.94 0 0.20 0.61 1.39 5311 1 intqRR[2] 0.84 0 0.18 0.54 1.25 5119 1 intqRR[3] 0.81 0 0.19 0.49 1.24 5302 1 Samples were drawn using NUTS(diag_e) at Fri Jan 24 03:34:39 2025. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). We can compare this model against the previous one to see which one highest ELPD value for better predictive performance. # extracting ELPD leave-one-out results from model with both continuous and categorical variables log_lik_with_cat &lt;- extract_log_lik(crr.negbin.model2, merge_chains = FALSE) r_eff_with_cat &lt;- relative_eff(log_lik_with_cat) loo_cat &lt;- loo(log_lik_with_cat, r_eff_with_cat, cores = 6) print(loo_cat) Computed from 9000 by 743 log-likelihood matrix. Estimate SE elpd_loo -1026.7 37.7 p_loo 7.7 1.0 looic 2053.4 75.4 ------ MCSE of elpd_loo is 0.0. MCSE and ESS estimates assume independent draws (r_eff=1). All Pareto k estimates are good (k &lt; 0.7). See help(&#39;pareto-k-diagnostic&#39;) for details. This first model’s ELPD was -1022.1, and full model’s show -1026.7. Based on this result, it is clear that the first model is better in terms of predictive performance since its ELPD is higher (i.e., -1022.1 vs -1026.7). So its best to stick with the first model, unless there is a strong theoretical reason to use second model (i.e., accounting for confounding). 4.3 Tasks 4.3.1 Task 1 - Obesity and Fastfoods in London Accessibility to junk food restaurants in young adolescents especially after school hours is a growing cause for concern. Especially, now that many young adults have a sedentary lifestyle; hence obesity rates among this population is increasing in the UK. Try this problem in Stan: Use the dataset Obesity and Fastfoods in MSOAs data.csv to determine the links between prevalence of obesity in high school students and density of fast food (cheap) restaurant and deprivation in MSOAs in London. Implement a Bayesian GLM using Stan code. Variable names: SEQID: ID number for row MSOA11CD: Unique identifier for MSOA area MSOA11NM: Name of the MSOA area OBESE: Number of child identified as obese in MSOA in London TOTAL: Total number of children surveyed for BMI measurements IMDMSOA: Area-level socioeconomic deprivation score (higher scores means higher deprivation and vice versa) RESTCAT: Categorical variable for classifying an MSOA in terms of density of junk/cheap fast food outlets restaurants: 1 = 1 to 10, 2= 11 to 25, 3= 26 to 50 and 4= 51 to 300. HINT: You might want to consider using the following functions: binomial_logit() or binomial() in the model block and reporting the odd ratios using the generated quantities block. You might want to consider computing the exceedance probabilities for the odd ratios using the threshold of 1. 4.3.2 Task 2 - Factors affecting house prices in London (2015) Try this problem in Stan: Use London LSOA 2015 data.csv data pertained house prices in 2015, and assess it’s relationship with public transport accessibility (PTA), average income and socioeconomic deprivation (IMD) as the independent variables. Implement a Bayesian GLM using Stan code. Variables names: LSOACODE: Unique identification code for the geographic area AVEPRICE: (Dependent variable) Average house price estimated for the LSOA in 2015 AVEINCOME: Estimated average annual income for households within an LSOA in 2015 IMDSCORE: Deprivation score for an LSOA in 2015 PTAINDEX: Measures levels of access/connectivity to public transport HINT: You might want to consider using the following functions: normal() in the model block. You might want to consider computing the exceedance probabilities for the coefficients using the threshold of 0. "],["bayesian-hierarchical-regression-models.html", "5 Bayesian Hierarchical Regression Models 5.1 Introduction 5.2 Data preparation and set-up for Bayesian analysis in Stan 5.3 Task", " 5 Bayesian Hierarchical Regression Models 5.1 Introduction 5.1.1 Lectures (Length: 1:04:43) [Watch on YouTube] 5.1.2 Learning outcomes We will learn how to perform hierarchical regression modelling within a Bayesian framework. These models are useful and robust especially if there’s a hierarchical structure in the dataset. This artefact must be taken into account to ensure statistical robustness. We will focus on implementing 2-level hierarchical regressions with an example of a model that has a varying intercept and varying-slope coefficients. These models are quite simple to pull off in Stan once you get the gist of it. You can follow the live walkthrough demonstration, and then use the practical sessions to try the practical tutorials yourself by following the instructions and trying out the tasks. 5.1.3 Demonstration [Part I] (Length: 1:57:40) [Watch on YouTube] 5.1.4 Demonstration [Part II] (Length: 1:01:48) [Watch on YouTube] 5.1.5 Datasets &amp; setting up the work directory Go to your folder CPD-course and create a sub folder called “Day 4”. Here, we will store all our R &amp; Stan scripts and data files. Set your work directory to the Day 4 folder. For Windows, the code for setting the work directory will be: setwd(&quot;C:/Users/AccountName/Desktop/CPD-course/Day 4&quot;) For MAC, the code for setting the work directory will be: setwd(&quot;/Users/AccountName/Desktop/CPD-course/Day 4&quot;) The main dataset for this practical: WHO-AFRO Cholera 2000-17.csv The dataset for the task: Pregnancies and hospital data.csv Let us load this dataset into memory: # load up the data Maths_dataset &lt;- read.csv(&quot;WHO-AFRO Cholera 2000-17.csv&quot;) 5.1.6 Loading packages We need to load the installed packages including rstan and tidybayes: # load up packages library(&#39;rstan&#39;) library(&#39;tidybayes&#39;) library(&#39;tidyverse&#39;) Configure Stan: # configure Stan options(mc.cores = parallel::detectCores()) rstan_options(auto_write = TRUE) 5.2 Data preparation and set-up for Bayesian analysis in Stan We are going to fit a 2-level Hierarchical regression model on a count outcome i.e., Cholera and to see the impact of the following risk factors: coverage of no basic water services and no sanitation services. The WHO_AFRO_Cholera object contains a total of 13 sub-Saharan African countries who are part of the World Health Organization, whereby each country has cholera, basic water and sanitation service information starting from 2000, and up to 2017, inclusive (i.e., 18 years worth of data). Other independent variables for the countries for each year includes GDP, Temperature and Rainfall. Note that the yearly data are grouped within the 13 countries country_id or country_name. Within a country, the information has a unique reading captured by the year_id. We want fit a 2-level negative binomial Poisson regression model with varying-intercept and varying-slopes on the basic water and sanitation services variables. To explore: Global relationships between the increased burden of limited water and basic sanitation services, and risk of cholera, across the WHO-AFRO. Local relationships between the increased burden of limited water and basic sanitation services, and risk of cholera, within each country. To perform such 2-level hierarchical model, we need to prepare the dataset in RStudio accordingly by extracting the right pieces of information from the WHO_AFRO_Cholera data frame object to be passed to the data block in our Stan script. KEY INGREDIENTS: N: total number of observations/rows in our dataset (234); Country: maximum number of groups formed in the dataset (13 countries); CountryID: this lists all the unique ID numbers for the group i.e., aligns country ID with the observations; Cholera: we want to extract this as a dependent variable and treat as an int because its counts measure; Water: we want to extract this as a primary independent variable and treat as an real since its a continuous measure; Sanitation: we want to extract the primary independent variable and treat as a real since its a continuous measure; GDP: we want to include this as an apriori independent variable and treat as an real since its a continuous measure; Rainfall: we want to include this as another apriori independent variable and treat as an real since its a continuous measure; Temperature: we want to include this as another apriori independent variable, just like the first two, and treat as an real since its a continuous measure; Log_Population: The population column has been log-transformed in the list(). This is going to be treated as an offset real measure; phi: Represent the dispersion in counts of cholera. We are going to assume that it is the same for all countries. We will assume their prior is from a beta distribution centred on around 0.2-0.3 (i.e., phi ~ beta(2, 5)). We can condense this information into list() object in line of code as our dataset for Stan to compile: # create dataset for Stan stan.cholera.dataset &lt;- list( N=nrow(WHO_AFRO_Cholera), Country=max(unique(WHO_AFRO_Cholera$country_id)), CountryID=as.integer(WHO_AFRO_Cholera$country_id), Cholera=WHO_AFRO_Cholera$cholera, Log_Population = log(WHO_AFRO_Cholera$population), Water = WHO_AFRO_Cholera$basic_water, Sanitation = WHO_AFRO_Cholera$basic_sanitation, GDP = WHO_AFRO_Cholera$gdp, Rainfall = WHO_AFRO_Cholera$rainfall, Temperature = WHO_AFRO_Cholera$temperature ) At this point, the dataset is now fully prepared and ready to go. Let’s create our Stan script for running a 2-level negative binomial Poisson regression within a Bayesian framework. 5.2.1 Creating a script to run a 2-level Multilevel linear regression in Stan We will need data, parameters, transformed parameters and model blocks for this analysis. FIRST STEP: In the data block, we can pass the key information from our list object stan.cholera.dataset. data { int&lt;lower=1&gt; N; // Number of observations int&lt;lower=1&gt; Country; // Number of countries array[N] int&lt;lower=1, upper=Country&gt; CountryID; // Country IDs array[N] int&lt;lower=0&gt; Cholera; // Cholera cases array[N] real Water; // Water access variable array[N] real Sanitation; // Sanitation variable array[N] real GDP; // GDP variable array[N] real Rainfall; // Rainfall variable array[N] real Temperature; // Temperature variable array[N] real Log_Population; // Logged Population variable used as offset } SECOND STEP: For the parameters block, here we will need to specify the name of the fixed effect of the regression model i.e., gamma00, gamma01 and gamma02, which is the overall intercept, and the overall effect (coefficients) of Water and Sanitation on Cholera, respectively. These are operating on the level-1 part of the model. We want the intercept and latter coefficients vary across the countries at level-2; thus, we define country-specific random effects from them which is governed by some standard deviation i.e., their variation within countries. Note that the coefficients beta3, beta4 and beta5 for GDP, Rainfall and Temperature are on level-1. Lastly, we are defining phi for over-dispersed. Here is the code: data { int&lt;lower=1&gt; N; // Number of observations int&lt;lower=1&gt; Country; // Number of countries array[N] int&lt;lower=1, upper=Country&gt; CountryID; // Country IDs array[N] int&lt;lower=0&gt; Cholera; // Cholera cases array[N] real Water; // Water access variable array[N] real Sanitation; // Sanitation variable array[N] real GDP; // GDP variable array[N] real Rainfall; // Rainfall variable array[N] real Temperature; // Temperature variable array[N] real Log_Population; // Logged Population variable used as offset } parameters { real gamma00; // Overall intercept real gamma01; // Overall effect of Water real gamma02; // Overall effect of Sanitation real beta3; // overall fixed effects relationship with GDP real beta4; // overall fixed effects relationship with rainfall real beta5; // overall fixed effects relationship with temperature array[Country] real random_intercept; // Country-specific random intercepts array[Country] real random_slope_water; // Country-specific random slopes for Water array[Country] real random_slope_sanitation; // Country-specific random slopes for Sanitation real&lt;lower=0&gt; group_intercept_sd; // SD of random intercepts real&lt;lower=0&gt; group_slope_water_sd; // SD of random slopes for Water real&lt;lower=0&gt; group_slope_sanitation_sd; // SD of random slopes for Sanitation real&lt;lower=0&gt; phi; // Use phi for dispersion } THIRD STEP: For the transformed parameters block, include the sub-equations for beta00, beta01 and beta02, where we add the fixed effects and random effects: data { int&lt;lower=1&gt; N; // Number of observations int&lt;lower=1&gt; Country; // Number of countries array[N] int&lt;lower=1, upper=Country&gt; CountryID; // Country IDs array[N] int&lt;lower=0&gt; Cholera; // Cholera cases array[N] real Water; // Water access variable array[N] real Sanitation; // Sanitation variable array[N] real GDP; // GDP variable array[N] real Rainfall; // Rainfall variable array[N] real Temperature; // Temperature variable array[N] real Log_Population; // Logged Population variable used as offset } parameters { real gamma00; // Overall intercept real gamma01; // Overall effect of Water real gamma02; // Overall effect of Sanitation real beta3; // overall fixed effects relationship with GDP real beta4; // overall fixed effects relationship with rainfall real beta5; // overall fixed effects relationship with temperature array[Country] real random_intercept; // Country-specific random intercepts array[Country] real random_slope_water; // Country-specific random slopes for Water array[Country] real random_slope_sanitation; // Country-specific random slopes for Sanitation real&lt;lower=0&gt; group_intercept_sd; // SD of random intercepts real&lt;lower=0&gt; group_slope_water_sd; // SD of random slopes for Water real&lt;lower=0&gt; group_slope_sanitation_sd; // SD of random slopes for Sanitation real&lt;lower=0&gt; phi; // Use phi for dispersion } transformed parameters { array[Country] real beta00; array[Country] real beta01; array[Country] real beta02; for (j in 1:Country) { beta00[j] = gamma00 + random_intercept[j]; // Random intercept per country - overall risks of cholera varying by country beta01[j] = gamma01 + random_slope_water[j]; // Random slope for Water - overall risks cholera with `Water` varying by country beta02[j] = gamma02 + random_slope_sanitation[j]; // Random slope for Sanitation - overall risks cholera with `Sanitation` varying by country } } FOURTH STEP: We build our likelihood function and specify the priors for each parameter under the model block. This will contain our statistical model. data { int&lt;lower=1&gt; N; // Number of observations int&lt;lower=1&gt; Country; // Number of countries array[N] int&lt;lower=1, upper=Country&gt; CountryID; // Country IDs array[N] int&lt;lower=0&gt; Cholera; // Cholera cases array[N] real Water; // Water access variable array[N] real Sanitation; // Sanitation variable array[N] real GDP; // GDP variable array[N] real Rainfall; // Rainfall variable array[N] real Temperature; // Temperature variable array[N] real Log_Population; // Logged Population variable used as offset } parameters { real gamma00; // Overall intercept real gamma01; // Overall effect of Water real gamma02; // Overall effect of Sanitation real beta3; // overall fixed effects relationship with GDP real beta4; // overall fixed effects relationship with rainfall real beta5; // overall fixed effects relationship with temperature array[Country] real random_intercept; // Country-specific random intercepts array[Country] real random_slope_water; // Country-specific random slopes for Water array[Country] real random_slope_sanitation; // Country-specific random slopes for Sanitation real&lt;lower=0&gt; group_intercept_sd; // SD of random intercepts real&lt;lower=0&gt; group_slope_water_sd; // SD of random slopes for Water real&lt;lower=0&gt; group_slope_sanitation_sd; // SD of random slopes for Sanitation real&lt;lower=0&gt; phi; // Use phi for dispersion } transformed parameters { array[Country] real beta00; array[Country] real beta01; array[Country] real beta02; for (j in 1:Country) { beta00[j] = gamma00 + random_intercept[j]; // Random intercept per country - overall risks of cholera varying by country beta01[j] = gamma01 + random_slope_water[j]; // Random slope for Water - overall risks cholera with `Water` varying by country beta02[j] = gamma02 + random_slope_sanitation[j]; // Random slope for Sanitation - overall risks cholera with `Sanitation` varying by country } } model { // Priors for fixed effects gamma00 ~ normal(0, 1); gamma01 ~ normal(0, 1); gamma02 ~ normal(0, 1); beta3 ~ normal(0, 1); beta4 ~ normal(0, 1); beta5 ~ normal(0, 1); // Priors for random effects random_intercept ~ normal(0, group_intercept_sd); random_slope_water ~ normal(0, group_slope_water_sd); random_slope_sanitation ~ normal(0, group_slope_sanitation_sd); // Priors for standard deviations of random effects group_intercept_sd ~ cauchy(0, 0.5); group_slope_water_sd ~ cauchy(0, 0.5); group_slope_sanitation_sd ~ cauchy(0, 0.5); // Prior for overdispersion parameter phi ~ beta(2, 5); // Likelihood: Negative Binomial Poisson Regression for (i in 1:N) { Cholera[i] ~ neg_binomial_2_log(beta00[CountryID[i]] + beta01[CountryID[i]]*Water[i] + beta02[CountryID[i]]*Sanitation[i] + beta3*GDP[i] + beta4*Rainfall[i] + beta5*Temperature[i] + Log_Population[i], phi); } } FINAL STEP: We include generated quantities block to derive all results as relative risk ratios. data { int&lt;lower=1&gt; N; // Number of observations int&lt;lower=1&gt; Country; // Number of countries array[N] int&lt;lower=1, upper=Country&gt; CountryID; // Country IDs array[N] int&lt;lower=0&gt; Cholera; // Cholera cases array[N] real Water; // Water access variable array[N] real Sanitation; // Sanitation variable array[N] real GDP; // GDP variable array[N] real Rainfall; // Rainfall variable array[N] real Temperature; // Temperature variable array[N] real Log_Population; // Logged Population variable used as offset } parameters { real gamma00; // Overall intercept real gamma01; // Overall effect of Water real gamma02; // Overall effect of Sanitation real beta3; // overall fixed effects relationship with GDP real beta4; // overall fixed effects relationship with rainfall real beta5; // overall fixed effects relationship with temperature array[Country] real random_intercept; // Country-specific random intercepts array[Country] real random_slope_water; // Country-specific random slopes for Water array[Country] real random_slope_sanitation; // Country-specific random slopes for Sanitation real&lt;lower=0&gt; group_intercept_sd; // SD of random intercepts real&lt;lower=0&gt; group_slope_water_sd; // SD of random slopes for Water real&lt;lower=0&gt; group_slope_sanitation_sd; // SD of random slopes for Sanitation real&lt;lower=0&gt; phi; // Use phi for dispersion } transformed parameters { array[Country] real beta00; array[Country] real beta01; array[Country] real beta02; for (j in 1:Country) { beta00[j] = gamma00 + random_intercept[j]; // Random intercept per country - overall risks of cholera varying by country beta01[j] = gamma01 + random_slope_water[j]; // Random slope for Water - overall risks cholera with `Water` varying by country beta02[j] = gamma02 + random_slope_sanitation[j]; // Random slope for Sanitation - overall risks cholera with `Sanitation` varying by country } } model { // Priors for fixed effects gamma00 ~ normal(0, 1); gamma01 ~ normal(0, 1); gamma02 ~ normal(0, 1); beta3 ~ normal(0, 1); beta4 ~ normal(0, 1); beta5 ~ normal(0, 1); // Priors for random effects random_intercept ~ normal(0, group_intercept_sd); random_slope_water ~ normal(0, group_slope_water_sd); random_slope_sanitation ~ normal(0, group_slope_sanitation_sd); // Priors for standard deviations of random effects group_intercept_sd ~ cauchy(0, 0.5); group_slope_water_sd ~ cauchy(0, 0.5); group_slope_sanitation_sd ~ cauchy(0, 0.5); // Prior for overdispersion parameter phi ~ beta(2, 5); // Likelihood: Negative Binomial Poisson Regression for (i in 1:N) { Cholera[i] ~ neg_binomial_2_log(beta00[CountryID[i]] + beta01[CountryID[i]]*Water[i] + beta02[CountryID[i]]*Sanitation[i] + beta3*GDP[i] + beta4*Rainfall[i] + beta5*Temperature[i] + Log_Population[i], phi); } } generated quantities { // report the coefficients as relative risk ratios real gamma00_RR; real gamma01_RR; real gamma02_RR; gamma00_RR = exp(gamma00); gamma01_RR = exp(gamma01); gamma02_RR = exp(gamma02); real beta3_RR; real beta4_RR; real beta5_RR; beta3_RR = exp(beta3); beta4_RR = exp(beta4); beta5_RR = exp(beta5); // report the varying slopes as relative risk ratios array[Country] real beta00_RR; array[Country] real beta01_RR; array[Country] real beta02_RR; beta00_RR = exp(beta00); beta01_RR = exp(beta01); beta02_RR = exp(beta02); } // end script Let’s save the script as Cholera Script.stan. Now, we can compile and run it through RStudio to get our estimated coefficients. 5.2.2 Compiling our Stan code for Hierarchical Modelling Now, let us turn our attention to RStudio. Using the stan() to compile the save script to obtain the posterior estimation of the parameters in our hierarchical model: # Start the clock ptm &lt;- proc.time() # compile linear regression model for now bayesian.hierarchical.model = stan(&quot;Cholera Script.stan&quot;, data=stan.dataset, iter=3000, chains=3, verbose = FALSE) # Stop the clock proc.time() - ptm Output summary table # print full table to avoid some rows from being omitted. options(max.print = 100000) # print full table to see all results print(bayesian.hierarchical.model) #PRINTED OUTPUT FROM CONSOLE Inference for Stan model: anon_model. 3 chains, each with iter=3000; warmup=1500; thin=1; post-warmup draws per chain=1500, total post-warmup draws=4500. mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat gamma00 -0.52 0.02 0.95 -2.38 -1.16 -0.53 0.11 1.37 3059 1.00 gamma01 0.59 0.01 0.30 -0.03 0.42 0.61 0.79 1.16 1780 1.00 gamma02 0.39 0.01 0.42 -0.39 0.11 0.37 0.66 1.25 1841 1.00 beta3 0.27 0.01 0.20 -0.13 0.14 0.27 0.41 0.65 1363 1.00 beta4 -0.01 0.00 0.01 -0.02 -0.01 -0.01 -0.01 0.00 2878 1.00 beta5 -0.30 0.00 0.04 -0.38 -0.33 -0.30 -0.27 -0.21 2767 1.00 random_intercept[1] -0.04 0.01 0.34 -0.84 -0.15 -0.01 0.08 0.63 2975 1.00 random_intercept[2] -0.02 0.01 0.34 -0.79 -0.14 -0.01 0.11 0.69 4060 1.00 random_intercept[3] 0.09 0.01 0.33 -0.47 -0.06 0.03 0.21 0.92 2012 1.00 random_intercept[4] 0.05 0.01 0.35 -0.62 -0.08 0.01 0.17 0.89 4091 1.00 random_intercept[5] 0.06 0.01 0.33 -0.57 -0.08 0.02 0.17 0.83 2752 1.00 random_intercept[6] -0.13 0.01 0.38 -1.13 -0.25 -0.05 0.05 0.47 984 1.00 random_intercept[7] -0.02 0.00 0.26 -0.59 -0.13 -0.01 0.09 0.56 4029 1.00 random_intercept[8] -0.07 0.01 0.28 -0.75 -0.19 -0.03 0.06 0.47 2355 1.00 random_intercept[9] -0.06 0.01 0.36 -0.95 -0.17 -0.02 0.09 0.61 3042 1.00 random_intercept[10] 0.02 0.01 0.34 -0.68 -0.11 0.00 0.13 0.77 3193 1.00 random_intercept[11] 0.14 0.01 0.37 -0.43 -0.04 0.05 0.25 1.14 1049 1.00 random_intercept[12] 0.01 0.00 0.25 -0.50 -0.10 0.00 0.11 0.59 2753 1.00 random_intercept[13] -0.11 0.01 0.35 -1.02 -0.23 -0.04 0.06 0.47 1403 1.00 random_slope_water[1] 0.20 0.01 0.56 -0.80 -0.09 0.11 0.45 1.58 2275 1.00 random_slope_water[2] 0.10 0.01 0.57 -1.05 -0.18 0.06 0.37 1.38 2779 1.00 random_slope_water[3] 0.14 0.01 0.46 -0.78 -0.09 0.09 0.38 1.16 2596 1.00 random_slope_water[4] -0.31 0.01 0.44 -1.33 -0.55 -0.23 -0.01 0.42 1034 1.00 random_slope_water[5] -0.07 0.01 0.42 -0.91 -0.30 -0.06 0.12 0.87 1882 1.00 random_slope_water[6] -0.06 0.01 0.60 -1.47 -0.31 -0.01 0.23 1.15 4630 1.00 random_slope_water[7] 0.47 0.03 0.64 -0.44 0.02 0.31 0.79 2.11 605 1.00 random_slope_water[8] -0.12 0.01 0.43 -1.06 -0.34 -0.07 0.11 0.71 2222 1.00 random_slope_water[9] -0.23 0.02 0.63 -1.84 -0.51 -0.10 0.12 0.85 1610 1.00 random_slope_water[10] -0.56 0.03 0.74 -2.39 -0.95 -0.35 -0.03 0.42 569 1.00 random_slope_water[11] 0.17 0.01 0.40 -0.62 -0.06 0.12 0.39 1.02 2011 1.00 random_slope_water[12] -0.22 0.01 0.54 -1.46 -0.48 -0.12 0.08 0.73 1628 1.00 random_slope_water[13] 0.65 0.03 0.78 -0.29 0.08 0.41 1.06 2.59 516 1.00 random_slope_sanitation[1] 0.22 0.01 0.66 -1.05 -0.22 0.19 0.64 1.57 2253 1.00 random_slope_sanitation[2] 0.31 0.01 0.50 -0.72 -0.01 0.32 0.64 1.24 2156 1.00 random_slope_sanitation[3] 0.47 0.02 1.38 -2.12 -0.40 0.40 1.28 3.35 3419 1.00 random_slope_sanitation[4] 0.21 0.02 0.73 -1.23 -0.28 0.21 0.67 1.66 1610 1.00 random_slope_sanitation[5] 2.03 0.03 1.04 0.11 1.31 2.00 2.70 4.17 1612 1.00 random_slope_sanitation[6] 1.45 0.02 0.78 -0.06 0.94 1.46 1.96 2.96 1491 1.00 random_slope_sanitation[7] 1.13 0.03 1.33 -1.09 0.19 1.02 1.92 4.06 1521 1.00 random_slope_sanitation[8] 0.10 0.01 0.84 -1.47 -0.45 0.09 0.64 1.80 3218 1.00 random_slope_sanitation[9] -1.12 0.02 0.78 -2.74 -1.62 -1.11 -0.60 0.36 1669 1.00 random_slope_sanitation[10] -1.29 0.02 0.74 -2.75 -1.77 -1.29 -0.79 0.12 2215 1.00 random_slope_sanitation[11] -1.29 0.02 0.70 -2.73 -1.75 -1.28 -0.80 0.02 1707 1.00 random_slope_sanitation[12] -0.99 0.02 0.69 -2.34 -1.46 -0.98 -0.52 0.34 1670 1.00 random_slope_sanitation[13] -0.17 0.01 0.64 -1.42 -0.58 -0.18 0.24 1.08 1936 1.00 group_intercept_sd 0.28 0.02 0.24 0.02 0.10 0.22 0.40 0.88 179 1.00 group_slope_water_sd 0.57 0.02 0.39 0.06 0.27 0.50 0.78 1.48 313 1.01 group_slope_sanitation_sd 1.33 0.01 0.47 0.56 1.01 1.27 1.58 2.37 1202 1.00 phi 0.46 0.00 0.04 0.39 0.43 0.46 0.49 0.54 4832 1.00 beta00[1] -0.56 0.02 1.02 -2.59 -1.23 -0.57 0.11 1.44 3256 1.00 beta00[2] -0.54 0.02 1.00 -2.51 -1.19 -0.55 0.13 1.46 3161 1.00 beta00[3] -0.43 0.02 1.02 -2.45 -1.11 -0.43 0.25 1.61 2975 1.00 beta00[4] -0.47 0.02 1.03 -2.46 -1.15 -0.48 0.20 1.61 3083 1.00 beta00[5] -0.46 0.02 1.01 -2.47 -1.14 -0.47 0.21 1.58 2919 1.00 beta00[6] -0.65 0.02 1.03 -2.75 -1.33 -0.64 0.02 1.36 2771 1.00 beta00[7] -0.54 0.02 0.97 -2.45 -1.19 -0.53 0.10 1.41 3073 1.00 beta00[8] -0.59 0.02 0.99 -2.55 -1.25 -0.59 0.07 1.35 3069 1.00 beta00[9] -0.58 0.02 1.03 -2.64 -1.25 -0.58 0.11 1.43 3066 1.00 beta00[10] -0.50 0.02 1.02 -2.54 -1.19 -0.51 0.16 1.52 3219 1.00 beta00[11] -0.39 0.02 1.03 -2.37 -1.08 -0.40 0.29 1.66 2592 1.00 beta00[12] -0.51 0.02 0.98 -2.41 -1.16 -0.52 0.13 1.41 3030 1.00 beta00[13] -0.63 0.02 1.02 -2.65 -1.30 -0.62 0.04 1.36 2886 1.00 beta01[1] 0.80 0.01 0.58 -0.33 0.46 0.76 1.10 2.14 2975 1.00 beta01[2] 0.70 0.01 0.61 -0.53 0.38 0.69 1.01 2.02 3754 1.00 beta01[3] 0.74 0.01 0.45 -0.20 0.48 0.74 1.01 1.64 2684 1.00 beta01[4] 0.29 0.02 0.49 -0.82 -0.02 0.35 0.63 1.11 874 1.00 beta01[5] 0.52 0.01 0.37 -0.25 0.30 0.54 0.76 1.27 3294 1.00 beta01[6] 0.53 0.01 0.67 -1.03 0.22 0.59 0.91 1.80 3007 1.00 beta01[7] 1.07 0.02 0.61 0.09 0.64 0.97 1.40 2.54 754 1.00 beta01[8] 0.48 0.01 0.43 -0.47 0.22 0.51 0.76 1.27 2051 1.00 beta01[9] 0.37 0.02 0.70 -1.36 0.02 0.49 0.80 1.55 1284 1.00 beta01[10] 0.04 0.03 0.83 -2.02 -0.39 0.24 0.62 1.22 564 1.00 beta01[11] 0.76 0.01 0.39 -0.01 0.52 0.75 1.00 1.54 2399 1.00 beta01[12] 0.37 0.02 0.58 -0.99 0.07 0.45 0.74 1.37 1178 1.00 beta01[13] 1.25 0.03 0.77 0.16 0.70 1.07 1.68 3.06 600 1.00 beta02[1] 0.61 0.01 0.58 -0.43 0.21 0.57 0.96 1.86 2941 1.00 beta02[2] 0.70 0.00 0.27 0.14 0.53 0.70 0.87 1.21 4444 1.00 beta02[3] 0.86 0.03 1.45 -1.77 -0.07 0.78 1.71 3.90 3171 1.00 beta02[4] 0.60 0.02 0.64 -0.65 0.16 0.60 1.02 1.88 1665 1.00 beta02[5] 2.42 0.03 1.07 0.37 1.68 2.42 3.13 4.54 1687 1.00 beta02[6] 1.84 0.02 0.74 0.34 1.36 1.86 2.34 3.22 1423 1.00 beta02[7] 1.52 0.03 1.39 -0.84 0.53 1.39 2.38 4.57 1819 1.00 beta02[8] 0.49 0.01 0.81 -1.07 -0.05 0.48 1.00 2.13 3592 1.00 beta02[9] -0.74 0.02 0.68 -2.04 -1.19 -0.76 -0.30 0.66 1798 1.00 beta02[10] -0.90 0.02 0.68 -2.19 -1.36 -0.92 -0.46 0.50 2050 1.00 beta02[11] -0.91 0.01 0.58 -2.07 -1.28 -0.90 -0.52 0.20 1512 1.00 beta02[12] -0.60 0.02 0.62 -1.80 -1.01 -0.64 -0.23 0.75 1577 1.00 beta02[13] 0.22 0.01 0.53 -0.75 -0.13 0.20 0.55 1.32 2464 1.00 gamma00_RR 0.94 0.02 1.18 0.09 0.31 0.59 1.11 3.94 2838 1.00 gamma01_RR 1.89 0.01 0.56 0.97 1.52 1.84 2.20 3.18 2057 1.00 gamma02_RR 1.62 0.02 0.75 0.68 1.11 1.45 1.94 3.50 1786 1.00 beta3_RR 1.33 0.01 0.27 0.87 1.15 1.31 1.50 1.92 1474 1.00 beta4_RR 0.99 0.00 0.01 0.98 0.99 0.99 0.99 1.00 2879 1.00 beta5_RR 0.74 0.00 0.03 0.68 0.72 0.74 0.76 0.81 2764 1.00 beta00_RR[1] 0.96 0.02 1.32 0.08 0.29 0.56 1.12 4.22 3061 1.00 beta00_RR[2] 0.96 0.02 1.25 0.08 0.30 0.58 1.13 4.30 2994 1.00 beta00_RR[3] 1.10 0.03 1.49 0.09 0.33 0.65 1.29 4.99 2653 1.00 beta00_RR[4] 1.10 0.04 1.86 0.09 0.32 0.62 1.22 4.98 2758 1.00 beta00_RR[5] 1.08 0.03 1.63 0.08 0.32 0.63 1.23 4.85 2893 1.00 beta00_RR[6] 0.88 0.02 1.23 0.06 0.27 0.53 1.02 3.89 2897 1.00 beta00_RR[7] 0.94 0.02 1.23 0.09 0.30 0.59 1.11 4.09 2777 1.00 beta00_RR[8] 0.90 0.02 1.17 0.08 0.29 0.55 1.08 3.86 2696 1.00 beta00_RR[9] 0.94 0.02 1.28 0.07 0.29 0.56 1.12 4.18 3033 1.00 beta00_RR[10] 1.03 0.03 1.49 0.08 0.30 0.60 1.17 4.59 2829 1.00 beta00_RR[11] 1.19 0.04 1.84 0.09 0.34 0.67 1.34 5.27 2390 1.00 beta00_RR[12] 0.97 0.02 1.26 0.09 0.31 0.60 1.14 4.10 2782 1.00 beta00_RR[13] 0.91 0.03 1.54 0.07 0.27 0.54 1.04 3.91 2895 1.00 beta01_RR[1] 2.71 0.05 2.45 0.72 1.59 2.13 3.00 8.46 2095 1.00 beta01_RR[2] 2.45 0.04 2.07 0.59 1.46 1.99 2.76 7.51 2634 1.00 beta01_RR[3] 2.33 0.03 1.33 0.82 1.61 2.09 2.73 5.14 2365 1.00 beta01_RR[4] 1.49 0.02 0.68 0.44 0.98 1.42 1.87 3.05 1119 1.00 beta01_RR[5] 1.81 0.01 0.73 0.78 1.35 1.71 2.14 3.56 2784 1.00 beta01_RR[6] 2.13 0.03 1.84 0.36 1.24 1.80 2.49 6.07 3057 1.00 beta01_RR[7] 3.65 0.11 3.44 1.10 1.90 2.63 4.06 12.66 970 1.00 beta01_RR[8] 1.77 0.02 0.78 0.63 1.24 1.66 2.13 3.56 2439 1.00 beta01_RR[9] 1.78 0.03 1.21 0.26 1.02 1.63 2.22 4.73 2272 1.00 beta01_RR[10] 1.35 0.03 0.89 0.13 0.67 1.27 1.87 3.40 881 1.00 beta01_RR[11] 2.31 0.02 1.00 0.99 1.67 2.12 2.72 4.66 2519 1.00 beta01_RR[12] 1.69 0.02 0.99 0.37 1.07 1.58 2.09 3.95 2288 1.00 beta01_RR[13] 5.07 0.22 6.82 1.17 2.01 2.90 5.38 21.35 992 1.00 beta02_RR[1] 2.22 0.04 1.90 0.65 1.24 1.77 2.62 6.43 2363 1.00 beta02_RR[2] 2.08 0.01 0.58 1.15 1.69 2.02 2.39 3.36 4284 1.00 beta02_RR[3] 15.51 3.97 231.58 0.17 0.93 2.18 5.53 49.53 3406 1.00 beta02_RR[4] 2.24 0.04 1.68 0.52 1.17 1.81 2.77 6.53 2282 1.00 beta02_RR[5] 20.08 0.58 29.58 1.45 5.36 11.23 22.88 94.06 2588 1.00 beta02_RR[6] 8.16 0.15 6.58 1.40 3.89 6.44 10.33 25.02 2041 1.00 beta02_RR[7] 15.62 1.15 60.65 0.43 1.69 4.02 10.75 96.23 2794 1.00 beta02_RR[8] 2.35 0.06 3.46 0.34 0.95 1.61 2.73 8.45 3218 1.00 beta02_RR[9] 0.61 0.01 0.49 0.13 0.30 0.47 0.74 1.93 1537 1.00 beta02_RR[10] 0.52 0.01 0.42 0.11 0.26 0.40 0.63 1.66 1570 1.00 beta02_RR[11] 0.48 0.01 0.30 0.13 0.28 0.41 0.59 1.22 1224 1.00 beta02_RR[12] 0.67 0.02 0.56 0.17 0.37 0.53 0.80 2.11 1305 1.00 beta02_RR[13] 1.45 0.02 1.04 0.47 0.88 1.22 1.73 3.73 2128 1.00 lp__ -2050.22 1.22 14.88 -2076.40 -2060.84 -2051.37 -2040.26 -2019.30 149 1.00 Samples were drawn using NUTS(diag_e) at Thu Jun 12 05:52:03 2025. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). You can examine the traceplot of divergence if necessary: # print traceplot for the global coefficients traceplot(bayesian.hierarchical.model, pars = c(&quot;gamma00_RR&quot;, &quot;gamma01_RR&quot;, &quot;gamma02_RR&quot;, &quot;beta3_RR&quot;, &quot;beta4_RR&quot;, &quot;beta5_RR&quot;, &quot;group_intercept_sd&quot;, &quot;group_slope_water_sd&quot;, &quot;group_slope_sanitation_sd&quot;, &quot;phi&quot;)) # print traceplot for the varying intercept and varying coefficients results traceplot(bayesian.hierarchical.model, pars = c(&quot;beta00_RR&quot;, &quot;beta01_RR&quot;, &quot;beta02_RR&quot;)) Dealing with results from Stan can be very challenging. Let us show you how to extract the desired results. We will first report the overall relative risks and group variations: # print table to reports the relative risk ratios for Cholera print(bayesian.hierarchical.model, probs=c(0.025, 0.975), pars = c(&quot;gamma00_RR&quot;, &quot;gamma01_RR&quot;, &quot;gamma02_RR&quot;, &quot;beta3_RR&quot;, &quot;beta4_RR&quot;, &quot;beta5_RR&quot;, &quot;group_intercept_sd&quot;, &quot;group_slope_water_sd&quot;, &quot;group_slope_sanitation_sd&quot;)) # PRINTED OUTPUT Inference for Stan model: anon_model. 3 chains, each with iter=3000; warmup=1500; thin=1; post-warmup draws per chain=1500, total post-warmup draws=4500. mean se_mean sd 2.5% 97.5% n_eff Rhat gamma00_RR 0.94 0.02 1.18 0.09 3.94 2838 1.00 gamma01_RR 1.89 0.01 0.56 0.97 3.18 2057 1.00 gamma02_RR 1.62 0.02 0.75 0.68 3.50 1786 1.00 beta3_RR 1.33 0.01 0.27 0.87 1.92 1474 1.00 beta4_RR 0.99 0.00 0.01 0.98 1.00 2879 1.00 beta5_RR 0.74 0.00 0.03 0.68 0.81 2764 1.00 group_intercept_sd 0.28 0.02 0.24 0.02 0.88 179 1.00 group_slope_water_sd 0.57 0.02 0.39 0.06 1.48 313 1.01 group_slope_sanitation_sd 1.33 0.01 0.47 0.56 2.37 1202 1.00 Samples were drawn using NUTS(diag_e) at Thu Jun 12 05:52:03 2025. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). Here is the interpretation: gamma00 is the overall baseline risk of cholera in sub-Saharan Africa. It is 0.94 (95% CrI from 0.09 to 3.94) times lower than usual in the population. gamma01 is the overall risk of cholera associated with coverage without basic water services in sub-Saharan Africa. The risk of cholera is 1.89 (95% CrI: 0.97 to 3.18) times higher when coverage without such service gets higher. gamma02 is the overall risk of cholera associated with coverage without basic sanitation services in sub-Saharan Africa. The risk of cholera is 1.62 (95% CrI: 0.68 to 3.50) times higher when coverage without such service gets higher. group_intercept_sd: The standard deviation is 0.28 - this is the random intercepts across countries, which suggests that WHO-AFRO countries have somewhat low variability in their baseline cholera rates. This can be interpreted as countries don’t differ drastically in baseline cholera risk (this can be eyeballed from the non-exponentiated values from beta00[1], beta00[2] to beta00[13]). group_slope_water_sd: The standard deviation is 0.57 - this is the random slope for the water service, which suggests that its effect on cholera risk somewhat varies considerably across the countries and not stable unlike the baseline risk (0.57 vs. 0.28)(- this variation can be eyeballed from the exponentiated values of beta01_RR[1] to beta01_RR[13]). group_slope_santitation_sd: The standard deviation is 1.33 - this is the random slope for the sanitation services, which suggests that its effect on cholera risk substantially varies across the countries, more so than the intercept and water (- this variation can be eyeballed from the exponentiated values of beta02_RR[1] to beta02_RR[13]). This portion of the code computes the exceedance probabilities the overall results above: # This portion of the code computes the exceedance probabilities for the intercept &amp; each beta coefficient threshold.gamma00_RR &lt;- function(x){mean(x &gt; 1.00)} gamma00_RR.exc.probs &lt;- bayesian.hierarchical.model %&gt;% spread_draws(gamma00_RR) %&gt;% summarise(gamma00_RR=threshold.gamma00_RR(gamma00_RR)) %&gt;% pull(gamma00_RR) threshold.gamma01_RR &lt;- function(x){mean(x &gt; 1.00)} gamma01_RR.exc.probs &lt;- bayesian.hierarchical.model %&gt;% spread_draws(gamma01_RR) %&gt;% summarise(gamma01_RR=threshold.gamma01_RR(gamma01_RR)) %&gt;% pull(gamma01_RR) threshold.gamma02_RR &lt;- function(x){mean(x &gt; 1.00)} gamma02_RR.exc.probs &lt;- bayesian.hierarchical.model %&gt;% spread_draws(gamma02_RR) %&gt;% summarise(gamma02_RR=threshold.gamma02_RR(gamma02_RR)) %&gt;% pull(gamma02_RR) threshold.beta3_RR &lt;- function(x){mean(x &gt; 1.00)} beta3_RR.exc.probs &lt;- bayesian.hierarchical.model %&gt;% spread_draws(beta3_RR) %&gt;% summarise(beta3_RR=threshold.beta3_RR (beta3_RR)) %&gt;% pull(beta3_RR) threshold.beta4_RR &lt;- function(x){mean(x &gt; 1.00)} beta4_RR.exc.probs &lt;- bayesian.hierarchical.model %&gt;% spread_draws(beta4_RR) %&gt;% summarise(beta4_RR=threshold.beta4_RR(beta4_RR)) %&gt;% pull(beta4_RR) threshold.beta5_RR &lt;- function(x){mean(x &gt; 1.00)} beta5_RR.exc.probs &lt;- bayesian.hierarchical.model %&gt;% spread_draws(beta5_RR) %&gt;% summarise(beta5_RR=threshold.beta5_RR(beta5_RR)) %&gt;% pull(beta5_RR) # report exceedance probability results gamma00_RR.exc.probs [1] 0.296175 gamma01_RR.exc.probs [1] 0.963475 gamma02_RR.exc.probs [1] 0.84035 beta3_RR.exc.probs [1] 0.9022667 beta4_RR.exc.probs [1] 0.06706667 beta5_RR.exc.probs [1] 0 We have presented the overall results. What about those that vary across countries? Let put everything into a table - here, its just a matter of writing codes to extracting the results and presenting them in a manner to be use in a paper or assignment: # Create a vector of names for the results - for overall, and for those that are country-specific # note: any country with &#39;_w&#39; is specific for water service # note: any country with &#39;_s&#39; is specific for sanitation service names &lt;- c(&quot;baseline&quot;, &quot;water&quot;, &quot;sanitation&quot;, &quot;gdp&quot;, &quot;rainfall&quot;, &quot;temperature&quot;, &quot;ben_w&quot;, &quot;bur_w&quot;, &quot;drc_w&quot;, &quot;gha_w&quot;, &quot;ivc_w&quot;, &quot;ken_w&quot;, &quot;mal_w&quot;, &quot;moz_w&quot;, &quot;ner_w&quot;, &quot;nir_w&quot;, &quot;som_w&quot;, &quot;tan_w&quot;, &quot;tog_w&quot;, &quot;ben_s&quot;, &quot;bur_s&quot;, &quot;drc_s&quot;, &quot;gha_s&quot;, &quot;ivc_s&quot;, &quot;ken_s&quot;, &quot;mal_s&quot;, &quot;moz_s&quot;, &quot;ner_s&quot;, &quot;nir_s&quot;, &quot;som_s&quot;, &quot;tan_s&quot;, &quot;tog_s&quot;) Create a data frame with the desired results: # export selected output as a data frame into &#39;result&#39; object results &lt;- as.data.frame(summary(bayesian.hierarchical.model, probs=c(0.025, 0.975), pars = c(&quot;gamma00_RR&quot; , &quot;gamma01_RR&quot;, &quot;gamma02_RR&quot;, &quot;beta3_RR&quot;, &quot;beta4_RR&quot;, &quot;beta5_RR&quot;, &quot;beta01_RR&quot;, &quot;beta02_RR&quot;))$summary) Align the names to the results in the results object: results$variables &lt;- names row.names(results) &lt;- 1:nrow(results) Now, we are cleaning the table up, with appropriate renaming and re-positioning of columns, and rounding the estimates down to 2 decimal places. results &lt;- results[,c(8, 1, 4, 5, 6, 7)] results$mean &lt;- round(results$mean, 2) colnames(results)[2] &lt;- &quot;RelativeRisks&quot; colnames(results)[3] &lt;- &quot;lower95&quot; colnames(results)[4] &lt;- &quot;upper95&quot; colnames(results)[5] &lt;- &quot;ess&quot; colnames(results)[6] &lt;- &quot;rhat&quot; results$lower95&lt;- round(results$lower95, 2) results$upper95 &lt;- round(results$upper95, 2) results$ess &lt;- round(results$ess, 0) results$rhat &lt;- round(results$rhat, 2) We are going to stitch the exceedance probabilities into this table. Note, we have not calculated the exceedance probabilities for the varying slopes! This portion of the code computes that for you: # This portion of the code computes the exceedance probabilities for the varying slope coefficients for each country threshold.beta01_RR &lt;- function(x){mean(x &gt; 1.00)} beta01_RR.exc.probs &lt;- bayesian.hierarchical.model %&gt;% spread_draws(beta01_RR[i]) %&gt;% group_by(i) %&gt;% summarise(beta01_RR=threshold.beta01_RR(beta01_RR)) %&gt;% pull(beta01_RR) threshold.beta02_RR &lt;- function(x){mean(x &gt; 1.00)} beta02_RR.exc.probs &lt;- bayesian.hierarchical.model %&gt;% spread_draws(beta02_RR[i]) %&gt;% group_by(i) %&gt;% summarise(beta02_RR=threshold.beta02_RR(beta02_RR)) %&gt;% pull(beta02_RR) beta01_RR.exc.probs [1] 0.9328583 0.9021333 0.9555667 0.7321000 0.9221833 0.8309167 0.9854167 0.8771083 0.7624833 0.6029917 0.9712000 0.7726417 0.9893000 beta02_RR.exc.probs [1] 0.8711167 0.9891167 0.7319250 0.8213667 0.9921333 0.9918500 0.8769833 0.7330500 0.1448750 0.1002167 0.0541000 0.1655333 0.6480667 We create the table output with final results table &lt;- results table$RR_95CrI &lt;- paste(table$RelativeRisks, &quot; (95% CrI: &quot;, table$lower95, &quot; to &quot;, table$upper95, &quot;)&quot;, sep = &quot;&quot;) probs &lt;- c(gamma00_RR.exc.probs, gamma01_RR.exc.probs, gamma02_RR.exc.probs, beta3_RR.exc.probs, beta4_RR.exc.probs, beta5_RR.exc.probs, beta01_RR.exc.probs, beta02_RR.exc.probs) table$ExceedProbs &lt;- round(probs, 2) table$ESS_Rhat &lt;- paste(table$ess, &quot; (Rhat = &quot;, table$rhat, &quot; &lt; 1.05)&quot;, sep = &quot;&quot;) finaltable &lt;- table[,c(1,7,8,9)] We have the full results in the finaltable object. Let us use an example for gha_w which refers to the estimate random slope for limited water services and it effect on cholera risk in Ghana. The risk in relation to this variable is 1.49 (95% CrI: 0.42 to 3.11) times higher. Such chance of risk being greater than 1 in relation to this variable is 0.73 (73%). This result; however, is not significant. 5.3 Task Try your hand on this problem in Stan: Build a random intercept logit model using data on 1060 births to 501 mothers. The outcome of interest is whether the birth was delivered in a hospital or elsewhere. The predictors include the log of income loginc, the distance to the nearest hospital distance, and two indicators of mothers’s education: dropout for less than high school and college for college graduate, so high school or some college is the reference cell. Have ago at building this model. "],["bayesian-spatial-modelling-for-areal-data-in-stan.html", "6 Bayesian Spatial Modelling for Areal Data in Stan 6.1 Introduction 6.2 Data preparation in RStudio 6.3 Creating the script for the Spatial ICAR model 6.4 Compiling Stan code for the Spatial ICAR risk modelling 6.5 Task", " 6 Bayesian Spatial Modelling for Areal Data in Stan 6.1 Introduction 6.1.1 Lectures (TBC) [EMBED LECTURES VIDEO HERE] [Watch on YouTube] 6.1.2 Learning outcomes Today, you will learn how to apply spatial Bayesian models for risk assessments for areal-level discrete outcomes. This is a powerful tool used often in many applications e.g., spatial epidemiology, disaster risk reduction and environmental criminology and many more. This exercise will focus on casualty data resulting from road accidents by car users in the UK. Road traffic accidents &amp; injuries are a serious problem worldwide. Here, we will estimate the area-specific relative risks (RR) of casualties due to road accidents in local authority areas across England; and we will quantify the levels of uncertainty using a device called exceedance probabilities. You will learn how to: Implement the Spatial intrinsic conditional autoregressive model (ICAR) to areal data in RStan; How to use the ICAR model to predict the area-specific relative risks (RR) for areal units and how to determine whether the levels of such risks are statistically significant or not through the 95% credible intervals (95% CrI); How to determine the Exceedance Probability i.e., the probability that an area has an excess risk that exceeds a given risk threshold (e.g., RR &gt; 1 (null value)); You can follow the live walkthrough demonstration of the practical and follow the instructions in this recording at your own. 6.1.3 Demonstration (TBC) [EMBED DEMONSTRATION VIDEO HERE] [Watch on YouTube] 6.1.4 Loading packages We will need to install the following new packages: spdep: grants access to function as(, 'Spatial)` function to coerce non-spatial objects into a spatial object. tmap: grants access to GIS functions. However, we will need to use the previous version i.e., tmap 3.3-4 instead of tmap 4. install.packages(&quot;spdep&quot;) install.packages(&quot;remotes&quot;) remotes::install_version(&quot;tmap&quot;, version = &quot;3.3-4&quot;) Now, lets load all packages need specifically for this computer practical: # Load the packages with library() library(&quot;sf&quot;) library(&quot;tmap&quot;) library(&quot;spdep&quot;) library(&quot;rstan&quot;) library(&quot;geostan&quot;) library(&quot;SpatialEpi&quot;) library(&quot;tidybayes&quot;) library(&quot;tidyverse&quot;) Upon loading the rstan package, we highly recommend using this code to configure it with RStudio: options(mc.cores = parallel::detectCores()) rstan_options(auto_write = TRUE) This tells RStudio to use the multiple cores in your computer for parallel processing whenever Stan is being implemented. Every time you want to use Stan - make sure to load parallel::detectCores() and rstan_options code. 6.1.5 Datasets &amp; setting up the work directory Go to your folder CPD-course and create a sub folder called “Day 5”. Here, we will store all our R &amp; Stan scripts and data files. Set your work directory to the Day 5 folder. For Windows, the code for setting the work directory will be: setwd(&quot;C:/Users/AccountName/Desktop/CPD-course/Day 5&quot;) For MAC, the code for setting the work directory will be: setwd(&quot;/Users/AccountName/Desktop/CPD-course/Day 5&quot;) The dataset for this practical are: Road Casualties Data 2015-2020.csv England Local Authority Shapefile.shp England Regions Shapefile.shp The dataset were going to start of with is the Road Casualties Data 2015-2020.csv. This data file contains the following information: It contains the 307 local authority areas that operate in England. The names and codes are defined under the columns LAD21NM and LAD21CD respectively; It contains the following variables: Population, Casualties and IMDScore. The Casualties is the dependent variable, and IMDScore is the independent variable. We will need Population column to derive the expected number of road casualties to be used as an offset in the Bayesian model. The shapefile England Local Authority Shapefile.shp contains the boundaries for all 307 local authorities in England. The England Regions Shapefile.shp contains the boundaries for all 10 regions that make up England. Let us load these dataset to memory: # load the shape files england_LA_shp &lt;- read_sf(&quot;England Local Authority Shapefile.shp&quot;) england_Region_shp &lt;- read_sf(&quot;England Regions Shapefile.shp&quot;) # load in the cross-sectional road accident dataset road_accidents &lt;- read.csv(&quot;Road Casualties Data 2015-2020.csv&quot;) 6.2 Data preparation in RStudio 6.2.1 Calculation for expected numbers In order to estimate the risk of casualties due to road accidents at an LA-level in England, we will need to first obtain a column that contains estimates from expected number of road casualties. This is derived from the Population column which as denominators or reference population size which is multiplied to the overall incidence rates of road accidents to get the number of expected casualties for each LA area. You can use the expected() function to compute this column into the road_accident data frame # calculate the expected number of cases road_accidents$ExpectedNum &lt;- round(expected(population = road_accidents$Population, cases = road_accidents$Casualties, n.strata = 1), 0) This particular column ExpectedNum is important, it must be computed and used as an offset in our spatial model. 6.2.2 Converting the spatial adjacency matrix to nodes &amp; edges We will need to transform the image below into a list of nodes and edges accordingly as Stan can only identify adjacency with a set of paired nodes with edges that connect them. For instance, node1 is the index region and node2 is the list of neighbouring regions connected to the index region in node1 We can perform this by first merging in the road accident data to the LA-level shapefile. Once this action is completed, we will then need to coerce the spatial object to be from a simple features (i.e., sf) object to the spatial object (i.e., sp). Here is the code: # merge the attribute table to the shapefile spatial.data &lt;- merge(england_LA_shp, road_accidents, by.x = c(&quot;LAD21CD&quot;, &quot;LAD21NM&quot;), by.y = c(&quot;LAD21CD&quot;, &quot;LAD21NM&quot;), all.x = TRUE) # reordering the columns spatial.data &lt;- spatial.data[, c(3,1,2,4,5,7,6)] # need to be coerced into a spatial object sp.object &lt;- as(spatial.data, &quot;Spatial&quot;) Now, we are going to need the nodes and edges from the sp.object using the shape2mat() function - this changes it into a matrix object. From the matrix object, we will be able to prepare the data from spatial ICAR model using the prep_icar_data() function. Here, is the code: # needs to be coerced into a matrix object adjacencyMatrix &lt;- shape2mat(sp.object) # we extract the components for the ICAR model extractComponents &lt;- prep_icar_data(adjacencyMatrix) From the extractComponents object, we will need to extract the following contents: $group_size this is the number of areal units under observation listed in the shapefile (should be the same in the road accidents dataset) $node1 are index regions of interest $node2 are the other neighbouring regions that are connected to the index region of interest listed in node1 $n_edges creates the network as show area is connected to what neighbourhood. It’s still an adjacency matrix using the queen contiguity matrix but as a network. Here is the code for performing the extraction: n &lt;- as.numeric(extractComponents$group_size) nod1 &lt;- extractComponents$node1 nod2 &lt;- extractComponents$node2 n_edges &lt;- as.numeric(extractComponents$n_edges) Note that the information needed are stored in n, nod1, nod2 and n_edges. 6.2.3 Create the dataset to be compiled in Stan For the list step in the data preparation, we need to define the variables needed to be compiled in Stan. The outcome Casualties, independent variable IMDScore and offset variable ExpectedNum needs to be extracted into separate vectors. The data needs to be aligned with the areas in shapefile as the result will be churned to that order. So make sure the data is already linked in to the geometries! Here is the code: y &lt;- spatial.data$Casualties x &lt;- spatial.data$IMDScore e &lt;- spatial.data$ExpectedNum Now, we create our dataset for Stan: # put all components into a list object stan.spatial.dataset &lt;- list(N=n, N_edges=n_edges, node1=nod1, node2=nod2, Y=y, X=x, Off_set=e) The above information is going to be passed to Stan in the data block. Now, we are in the position to develop our spatial intrinsic conditional autoregressive (ICAR) model. Now open your Stan script and we begin. 6.3 Creating the script for the Spatial ICAR model 6.3.1 Data block In the data block, we specify the following: The total number of areal unit observations N as an integer (i.e., 307), this corresponds to n; The total number edges N_edges as an integer (i.e., 823), this corresponds to n_edges; The nodes1 based on the size of N_edges (i.e., 823) must be specified as an array to connect with nodes2, this corresponds to nod1; The nodes2 based on the size of N_edges (i.e., 823) must be specified as an array to connect with nodes1, this corresponds to nod2; We define our Y outcome (i.e., road accidents) as an array of size N (i.e., 307) which is an integer, this corresponds to y; We define our independent variables X as a vector of size N (i.e., 307), this corresponds to x; We must define our offset for the expected numbers Off_set as a vector of size N (i.e., 307), this corresponds to e; Here is what our modeldata block` would look like: data { int&lt;lower=0&gt; N; // number of spatial units or neighbourhoods int&lt;lower=0&gt; N_edges; // number of edges connecting adjacent areas using Queens contiguity array[N_edges] int&lt;lower=1, upper=N&gt; node1; // list of index areas showing which spatial units are neighbours array[N_edges] int&lt;lower=1, upper=N&gt; node2; // list of neighbouring areas showing the connection to index spatial unit array[N] int&lt;lower=0&gt; Y; // dependent variable vector&lt;lower=0&gt;[N] X; // Single independent variable vector&lt;lower=0&gt;[N] Off_set; // offset variable } 6.3.2 Transformed data block We are going to include a transformed data block. Here, we are simply changing the expected numbers by taking its log() and creating another vector called log_offset. This will be added to the poisson_log() sampling statement in our likelihood function of the spatial model to account for the reference population in England. Here, we specify it as follows: data { int&lt;lower=0&gt; N; // number of spatial units or neighbourhoods int&lt;lower=0&gt; N_edges; // number of edges connecting adjacent areas using Queens contiguity array[N_edges] int&lt;lower=1, upper=N&gt; node1; // list of index areas showing which spatial units are neighbours array[N_edges] int&lt;lower=1, upper=N&gt; node2; // list of neighbouring areas showing the connection to index spatial unit array[N] int&lt;lower=0&gt; Y; // dependent variable vector&lt;lower=0&gt;[N] X; // Single independent variable vector&lt;lower=0&gt;[N] Off_set; // offset variable } transformed data { vector[N] log_Offset = log(Off_set); // use the expected cases as an offset and add to the regression model } 6.3.3 Parameters block For the parameters block, we will need to specify the following: The global intercept i.e., alpha for the entire study area (i.e., average risk of road accidents on a population-level); The coefficient beta for our independent variable X which is the IMDScore; We also specify sigma as a real value which is some error or standard deviation that is multiplied to the combined effects of our structured and unstructured random effects; We define the structured spatial random effects phi to be vector of size N; We define the unstructured spatial random effects theta to be vector of size N; We define rho as the proportion of the variation coming from the structured spatial random effects; We add the parameters block as follows: data { int&lt;lower=0&gt; N; // number of spatial units or neighbourhoods int&lt;lower=0&gt; N_edges; // number of edges connecting adjacent areas using Queens contiguity array[N_edges] int&lt;lower=1, upper=N&gt; node1; // list of index areas showing which spatial units are neighbours array[N_edges] int&lt;lower=1, upper=N&gt; node2; // list of neighbouring areas showing the connection to index spatial unit array[N] int&lt;lower=0&gt; Y; // dependent variable vector&lt;lower=0&gt;[N] X; // Single independent variable vector&lt;lower=0&gt;[N] Off_set; // offset variable } transformed data { vector[N] log_Offset = log(Off_set); // use the expected cases as an offset and add to the regression model } parameters { real alpha; // intercept real beta; // covariates real&lt;lower=0&gt; sigma; // overall standard deviation real&lt;lower=0, upper=1&gt; rho; // proportion unstructured vs. spatially structured variance vector[N] theta; // unstructured random effects vector[N] phi; // structured spatial random effects } 6.3.4 Transformed parameters block Here, we calculate the combined random effects from the structured and unstructured component for our model: data { int&lt;lower=0&gt; N; // number of spatial units or neighbourhoods int&lt;lower=0&gt; N_edges; // number of edges connecting adjacent areas using Queens contiguity array[N_edges] int&lt;lower=1, upper=N&gt; node1; // list of index areas showing which spatial units are neighbours array[N_edges] int&lt;lower=1, upper=N&gt; node2; // list of neighbouring areas showing the connection to index spatial unit array[N] int&lt;lower=0&gt; Y; // dependent variable vector&lt;lower=0&gt;[N] X; // Single independent variable vector&lt;lower=0&gt;[N] Off_set; // offset variable } transformed data { vector[N] log_Offset = log(Off_set); // use the expected cases as an offset and add to the regression model } parameters { real alpha; // intercept real beta; // covariates real&lt;lower=0&gt; sigma; // overall standard deviation real&lt;lower=0, upper=1&gt; rho; // proportion unstructured vs. spatially structured variance vector[N] theta; // unstructured random effects vector[N] phi; // structured spatial random effects } transformed parameters { vector[N] combined; // values derived from adding the unstructure and structured effect of each area combined = sqrt(1 - rho) * theta + sqrt(rho) * phi; // formulation for the combined random effect } 6.3.5 Model block We build our likelihood function and specify the priors for each parameter under the model block. We are using a typical Poisson model with a log link function as we are assuming there’s some linear relationship between the road accident counts and IMD score, but here we are also taking into account the combined spatial and non-spatial random effects: data { int&lt;lower=0&gt; N; // number of spatial units or neighbourhoods int&lt;lower=0&gt; N_edges; // number of edges connecting adjacent areas using Queens contiguity array[N_edges] int&lt;lower=1, upper=N&gt; node1; // list of index areas showing which spatial units are neighbours array[N_edges] int&lt;lower=1, upper=N&gt; node2; // list of neighbouring areas showing the connection to index spatial unit array[N] int&lt;lower=0&gt; Y; // dependent variable vector&lt;lower=0&gt;[N] X; // Single independent variable vector&lt;lower=0&gt;[N] Off_set; // offset variable } transformed data { vector[N] log_Offset = log(Off_set); // use the expected cases as an offset and add to the regression model } parameters { real alpha; // intercept real beta; // covariates real&lt;lower=0&gt; sigma; // overall standard deviation real&lt;lower=0, upper=1&gt; rho; // proportion unstructured vs. spatially structured variance vector[N] theta; // unstructured random effects vector[N] phi; // structured spatial random effects } transformed parameters { vector[N] combined; // values derived from adding the unstructure and structured effect of each area combined = sqrt(1 - rho) * theta + sqrt(rho) * phi; // formulation for the combined random effect } model { Y ~ poisson_log(log_Offset + alpha + X * beta + combined * sigma); // likelihood function: multivariable Poisson ICAR regression model // setting priors alpha ~ normal(0.0, 1.0); // prior for alpha: weakly informative beta ~ normal(0.0, 1.0); // prior for betas: weakly informative theta ~ normal(0.0, 1.0); // prior for theta: weakly informative sigma ~ normal(0.0, 1.0); // prior for sigma: weakly informative rho ~ beta(0.5, 0.5); // prior for rho target += -0.5 * dot_self(phi[node1] - phi[node2]); // calculates the spatial weights sum(phi) ~ normal(0, 0.001 * N); // priors for phi } 6.3.6 Generated quantities block Lastly, we instruct Stan on the parameters we want to report. We want them as relative risk ratio (RR). We can use the generated quantities block to obtain these estimates by exponentiation of the ICAR regression model: data { int&lt;lower=0&gt; N; // number of spatial units or neighbourhoods int&lt;lower=0&gt; N_edges; // number of edges connecting adjacent areas using Queens contiguity array[N_edges] int&lt;lower=1, upper=N&gt; node1; // list of index areas showing which spatial units are neighbours array[N_edges] int&lt;lower=1, upper=N&gt; node2; // list of neighbouring areas showing the connection to index spatial unit array[N] int&lt;lower=0&gt; Y; // dependent variable vector&lt;lower=0&gt;[N] X; // Single independent variable vector&lt;lower=0&gt;[N] Off_set; // offset variable } transformed data { vector[N] log_Offset = log(Off_set); // use the expected cases as an offset and add to the regression model } parameters { real alpha; // intercept real beta; // covariates real&lt;lower=0&gt; sigma; // overall standard deviation real&lt;lower=0, upper=1&gt; rho; // proportion unstructured vs. spatially structured variance vector[N] theta; // unstructured random effects vector[N] phi; // structured spatial random effects } transformed parameters { vector[N] combined; // values derived from adding the unstructure and structured effect of each area combined = sqrt(1 - rho) * theta + sqrt(rho) * phi; // formulation for the combined random effect } model { Y ~ poisson_log(log_Offset + alpha + X * beta + combined * sigma); // likelihood function: multivariable Poisson ICAR regression model // setting priors alpha ~ normal(0.0, 1.0); // prior for alpha: weakly informative beta ~ normal(0.0, 1.0); // prior for betas: weakly informative theta ~ normal(0.0, 1.0); // prior for theta: weakly informative sigma ~ normal(0.0, 1.0); // prior for sigma: weakly informative rho ~ beta(0.5, 0.5); // prior for rho target += -0.5 * dot_self(phi[node1] - phi[node2]); // calculates the spatial weights sum(phi) ~ normal(0, 0.001 * N); // priors for phi } generated quantities { vector[N] eta = alpha + X * beta + combined * sigma; // compute eta and exponentiate into mu vector[N] rr_mu = exp(eta); // output the neighbourhood-specific relative risks in mu real rr_beta = exp(beta); // output the risk ratios for each coefficient real rr_alpha = exp(alpha); // output the risk ratios for the intercept } Well done! You have coded your first spatial risk model. Alright, let us save the script as icar_poisson_model.stan. We can now compile and run it through RStudio to get our posterior estimates as risk ratios (RR) for each areas. We can also get the exceedance probabilities. The next steps are easy from this point onwards. 6.4 Compiling Stan code for the Spatial ICAR risk modelling 6.4.1 Printing of the global results Now, let us turn our attention to RStudio. Using the stan() to compile the saved script to obtain the posterior estimation of the parameters from our model: # Start the clock ptm &lt;- proc.time() icar_poisson_fit = stan(&quot;icar_poisson_model.stan&quot;, data=stan.spatial.dataset, iter=20000, control = list(max_treedepth = 12), chains=6, verbose = FALSE) # Stop the clock proc.time() - ptm We can see our estimated results for alpha, beta and sigma: # remove that annoying scientific notation options(scipen = 999) summary(icar_poisson_fit, pars=c(&quot;alpha&quot;, &quot;beta&quot;, &quot;sigma&quot;), probs=c(0.025, 0.975))$summary Output from summary()$summary function: mean se_mean sd 2.5% 97.5% n_eff Rhat alpha 0.162965832 0.00052694848 0.084071562 -0.001856892 0.32898381 25454.344 1.000094 beta -0.009775948 0.00002545072 0.004085887 -0.017779845 -0.00177424 25773.456 1.000076 sigma 0.627153399 0.00078449468 0.052970350 0.538581209 0.74606811 4559.169 1.001467 Here is the interpretation: alpha is the global mean (or average) in the population under study. It means on average the road accident occurrence in England for the period 2015 to 2020 is 0.1629 (95% CrI: -0.00185 to 0.3289). If we take the exponent of this value i.e., exp(0.162965832) - we get the relative risks of road accidents which is 1.17 times higher in England (95% CrI: 0.998 to 1.38). The result is not significant as the null value of 1 exists between its lower and upper limits. beta is the coefficient for IMDScore. This means that its yields a decrease on average for road accidents throughout England for more deprived areas -0.0097 (95% CI: -0.0177 to -0.00177424). This is negligible negative relationship that is statistically significant; while it is significant, if we exponentiate these values its really close to the null value (1), so we can rule out this relationship! sigma is the overall standard deviation or global error. Note, we can view the spatial effects i.e., phi for each area using this code: # show first 6 rows only instead of the full 307 summary(icar_poisson_fit, pars=c(&quot;phi&quot;), probs=c(0.025, 0.975))$summary Alternatively, you can use the print() function to get a detailed output: # print full table to avoid some rows from being omitted. options(max.print = 100000) # print the results print(icar_poisson_fit, pars=c(&quot;alpha&quot;, &quot;beta&quot;, &quot;rr_alpha&quot;, &quot;rr_beta&quot;, &quot;rr_mu&quot;, &quot;sigma&quot;), probs=c(0.025, 0.975)) Output from print() function: Inference for Stan model: anon_model. 6 chains, each with iter=20000; warmup=10000; thin=1; post-warmup draws per chain=10000, total post-warmup draws=60000. mean se_mean sd 2.5% 97.5% n_eff Rhat alpha 0.16 0 0.08 0.00 0.33 25454 1 beta -0.01 0 0.00 -0.02 0.00 25773 1 sigma 0.63 0 0.05 0.54 0.75 4559 1 Samples were drawn using NUTS(diag_e) at Fri Mar 8 07:41:39 2024. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). 6.4.2 Rapid diagnostics of the rHATs Before mapping the relative risks, we must check if the any of the estimates i.e., alpha, beta, sigma and all phi exceed the rHAT value of 1.05. This is an indication that the iterations did not perform well if an rHAT for a parameter is above 1.05. We can do a rapid checks to see which parameter is valid or not by creating a binary variable of 1’s (Valid) and 0’s (Not valid). We can tabulate it to see the numbers: # diagnostic check on the rHats - put everything into a data frame diagnostic.checks &lt;- as.data.frame(summary(icar_poisson_fit, pars=c(&quot;alpha&quot;, &quot;beta&quot;, &quot;rr_alpha&quot;, &quot;rr_beta&quot;, &quot;rr_mu&quot;, &quot;sigma&quot;, &quot;phi&quot;, &quot;lp__&quot;), probs=c(0.025, 0.5, 0.975))$summary) # create binary variable diagnostic.checks$valid &lt;- ifelse(diagnostic.checks$Rhat &lt; 1.05, 1, 0) # tabulate it table(diagnostic.checks$valid) Everything is okay - all outputted parameters have an rHAT &lt; 1.05. We are free to generate our maps. NOTES: To avoid such complications, it is always to best to run about 10000, 15000 or more iterations. Usually, shorter iterations yield low effective sample sizes after thinning/warm-up samples are discarded, which in turn, may lead to complications that may cause the rHAT to be above 1.05. 6.4.3 Extraction of the area-specific relative risks If you run the following code: # show first 6 rows only instead of the full 307 head(summary(icar_poisson_fit, pars=c(&quot;rr_mu&quot;), probs=c(0.025, 0.975))$summary) We see the relative risk (RR) estimates for the first areas under the column mu with their corresponding credibility limits under the 2.5% and 97.5% column. We are going to extract this information into a data frame and applying the cleaning and renaming of columns accordingly: # extraction key posterior results for the generated quantities relativeRisk.results &lt;- as.data.frame(summary(icar_poisson_fit, pars=c(&quot;rr_mu&quot;), probs=c(0.025, 0.975))$summary) # now cleaning up this table up # first, insert clean row numbers to new data frame row.names(relativeRisk.results) &lt;- 1:nrow(relativeRisk.results) # second, rearrange the columns into order relativeRisk.results &lt;- relativeRisk.results[, c(1,4,5,7)] # third, rename the columns appropriately colnames(relativeRisk.results)[1] &lt;- &quot;rr&quot; colnames(relativeRisk.results)[2] &lt;- &quot;rrlower&quot; colnames(relativeRisk.results)[3] &lt;- &quot;rrupper&quot; colnames(relativeRisk.results)[4] &lt;- &quot;rHAT&quot; # view clean table head(relativeRisk.results) See clean table: rr rrlower rrupper rHAT valid 1 0.6295692 0.5759348 0.6859993 0.9999354 1 2 0.8043656 0.7550300 0.8555160 0.9999809 1 3 0.6518326 0.6068501 0.6991145 0.9999186 1 4 0.6292392 0.5920648 0.6674334 0.9999475 1 5 0.8885551 0.8284516 0.9505547 1.0000141 1 6 0.7971718 0.7451160 0.8513495 0.9999757 1 Insert these columns into the spatial.data object as follow: # now, we proceed to generate our risk maps # align the results to the areas in shapefile spatial.data$rr &lt;- relativeRisk.results[, &quot;rr&quot;] spatial.data$rrlower &lt;- relativeRisk.results[, &quot;rrlower&quot;] spatial.data$rrupper &lt;- relativeRisk.results[, &quot;rrupper&quot;] These relative will allow us to see the mapped risks of road accidents across local authorities in England. We also want a supporting map indicate whether the risks are significant or not. Here, we create an extra column in the spatial.data called Significance. # create categories to define if an area has significant increase or decrease in risk, or nothing all spatial.data$Significance &lt;- NA spatial.data$Significance[spatial.data$rrlower&lt;1 &amp; spatial.data$rrupper&gt;1] &lt;- 0 # NOT SIGNIFICANT spatial.data$Significance[spatial.data$rrlower==1 | spatial.data$rrupper==1] &lt;- 0 # NOT SIGNIFICANT spatial.data$Significance[spatial.data$rrlower&gt;1 &amp; spatial.data$rrupper&gt;1] &lt;- 1 # SIGNIFICANT INCREASE spatial.data$Significance[spatial.data$rrlower&lt;1 &amp; spatial.data$rrupper&lt;1] &lt;- -1 # SIGNIFICANT DECREASE 6.4.4 Mapping of RR and significance The next set of codes are all cosmetics for the creating our risk map for road accidents. Here is the code: # For map design for the relative risk -- you want to understand or get a handle on what the distribution for risks look like # this would inform you of how to create the labelling for the legends when make a map in tmap summary(spatial.data$rr) hist(spatial.data$rr) # creating the labels RiskCategorylist &lt;- c(&quot;&gt;0.0 to 0.25&quot;, &quot;0.26 to 0.50&quot;, &quot;0.51 to 0.75&quot;, &quot;0.76 to 0.99&quot;, &quot;1.00 &amp; &lt;1.01&quot;, &quot;1.01 to 1.10&quot;, &quot;1.11 to 1.25&quot;, &quot;1.26 to 1.50&quot;, &quot;1.51 to 1.75&quot;, &quot;1.76 to 2.00&quot;, &quot;2.01 to 3.00&quot;) # next, we are creating the discrete colour changes for my legends and want to use a divergent colour scheme # scheme ranges from extreme dark blues to light blues to white to light reds to extreme dark reds # you can pick your own colour choices by checking out this link [https://colorbrewer2.org] RRPalette &lt;- c(&quot;#65bafe&quot;,&quot;#98cffe&quot;,&quot;#cbe6fe&quot;,&quot;#dfeffe&quot;,&quot;white&quot;,&quot;#fed5d5&quot;,&quot;#fcbba1&quot;,&quot;#fc9272&quot;,&quot;#fb6a4a&quot;,&quot;#de2d26&quot;,&quot;#a50f15&quot;) # categorising the risk values to match the labelling in RiskCategorylist object spatial.data$RelativeRiskCat &lt;- NA spatial.data$RelativeRiskCat[spatial.data$rr&gt;= 0 &amp; spatial.data$rr &lt;= 0.25] &lt;- -4 spatial.data$RelativeRiskCat[spatial.data$rr&gt; 0.25 &amp; spatial.data$rr &lt;= 0.50] &lt;- -3 spatial.data$RelativeRiskCat[spatial.data$rr&gt; 0.50 &amp; spatial.data$rr &lt;= 0.75] &lt;- -2 spatial.data$RelativeRiskCat[spatial.data$rr&gt; 0.75 &amp; spatial.data$rr &lt; 1] &lt;- -1 spatial.data$RelativeRiskCat[spatial.data$rr&gt;= 1.00 &amp; spatial.data$rr &lt; 1.01] &lt;- 0 spatial.data$RelativeRiskCat[spatial.data$rr&gt;= 1.01 &amp; spatial.data$rr &lt;= 1.10] &lt;- 1 spatial.data$RelativeRiskCat[spatial.data$rr&gt; 1.10 &amp; spatial.data$rr &lt;= 1.25] &lt;- 2 spatial.data$RelativeRiskCat[spatial.data$rr&gt; 1.25 &amp; spatial.data$rr &lt;= 1.50] &lt;- 3 spatial.data$RelativeRiskCat[spatial.data$rr&gt; 1.50 &amp; spatial.data$rr &lt;= 1.75] &lt;- 4 spatial.data$RelativeRiskCat[spatial.data$rr&gt; 1.75 &amp; spatial.data$rr &lt;= 2.00] &lt;- 5 spatial.data$RelativeRiskCat[spatial.data$rr&gt; 2.00 &amp; spatial.data$rr &lt;= 10] &lt;- 6 # check to see if legend scheme is balanced - if a number is missing that categorisation is wrong! table(spatial.data$RelativeRiskCat) Generating the maps as a paneled output: # map of relative risk rr_map &lt;- tm_shape(spatial.data) + tm_fill(&quot;RelativeRiskCat&quot;, style = &quot;cat&quot;, title = &quot;Relavtive Risk&quot;, palette = RRPalette, labels = RiskCategorylist) + tm_shape(england_Region_shp) + tm_polygons(alpha = 0.05) + tm_text(&quot;name&quot;, size = &quot;AREA&quot;) + tm_layout(frame = FALSE, legend.outside = TRUE, legend.title.size = 0.8, legend.text.size = 0.7) + tm_compass(position = c(&quot;right&quot;, &quot;top&quot;)) + tm_scale_bar(position = c(&quot;right&quot;, &quot;bottom&quot;)) # map of significance regions sg_map &lt;- tm_shape(spatial.data) + tm_fill(&quot;Significance&quot;, style = &quot;cat&quot;, title = &quot;Significance Categories&quot;, palette = c(&quot;#33a6fe&quot;, &quot;white&quot;, &quot;#fe0000&quot;), labels = c(&quot;Significantly low&quot;, &quot;Not Significant&quot;, &quot;Significantly high&quot;)) + tm_shape(england_Region_shp) + tm_polygons(alpha = 0.10) + tm_text(&quot;name&quot;, size = &quot;AREA&quot;) + tm_layout(frame = FALSE, legend.outside = TRUE, legend.title.size = 0.8, legend.text.size = 0.7) + tm_compass(position = c(&quot;right&quot;, &quot;top&quot;)) + tm_scale_bar(position = c(&quot;right&quot;, &quot;bottom&quot;)) # create side-by-side plot tmap_arrange(rr_map, sg_map, ncol = 2, nrow = 1) Output: 6.4.5 Extracting and mapping of the exceedance probabilities Exceedance probabilities allows the user to quantify the levels of uncertainty surrounding the risks we quantified. We can use a threshold for instance an RR &gt; 1 and ask what is the probability that an area has an excess risk of road accidents and visualise this as well. Just like the RRs, we are going to extract this information into a vector and include it into our spatial.data object. For this extraction, we will need to use functions from the tidybayes and tidyverse packages i.e., spread_draws(), group_by(), summarise() and pull(): # extract the exceedence probabilities from the icar_possion_fit object # compute the probability that an area has a relative risk ratio &gt; 1.0 threshold &lt;- function(x){mean(x &gt; 1.00)} excProbrr &lt;- icar_poisson_fit %&gt;% spread_draws(rr_mu[i]) %&gt;% group_by(i) %&gt;% summarise(rr_mu=threshold(rr_mu)) %&gt;% pull(rr_mu) # insert the exceedance values into the spatial data frame spatial.data$excProb &lt;- excProbrr The next set of codes are all cosmetics for the creating our probability exceedance map for road accidents. Here is the code: # create the labels for the probabilities ProbCategorylist &lt;- c(&quot;&lt;0.01&quot;, &quot;0.01-0.09&quot;, &quot;0.10-0.19&quot;, &quot;0.20-0.29&quot;, &quot;0.30-0.39&quot;, &quot;0.40-0.49&quot;,&quot;0.50-0.59&quot;, &quot;0.60-0.69&quot;, &quot;0.70-0.79&quot;, &quot;0.80-0.89&quot;, &quot;0.90-0.99&quot;, &quot;1.00&quot;) # categorising the probabilities in bands of 10s spatial.data$ProbCat &lt;- NA spatial.data$ProbCat[spatial.data$excProb&gt;=0 &amp; spatial.data$excProb&lt; 0.01] &lt;- 1 spatial.data$ProbCat[spatial.data$excProb&gt;=0.01 &amp; spatial.data$excProb&lt; 0.10] &lt;- 2 spatial.data$ProbCat[spatial.data$excProb&gt;=0.10 &amp; spatial.data$excProb&lt; 0.20] &lt;- 3 spatial.data$ProbCat[spatial.data$excProb&gt;=0.20 &amp; spatial.data$excProb&lt; 0.30] &lt;- 4 spatial.data$ProbCat[spatial.data$excProb&gt;=0.30 &amp; spatial.data$excProb&lt; 0.40] &lt;- 5 spatial.data$ProbCat[spatial.data$excProb&gt;=0.40 &amp; spatial.data$excProb&lt; 0.50] &lt;- 6 spatial.data$ProbCat[spatial.data$excProb&gt;=0.50 &amp; spatial.data$excProb&lt; 0.60] &lt;- 7 spatial.data$ProbCat[spatial.data$excProb&gt;=0.60 &amp; spatial.data$excProb&lt; 0.70] &lt;- 8 spatial.data$ProbCat[spatial.data$excProb&gt;=0.70 &amp; spatial.data$excProb&lt; 0.80] &lt;- 9 spatial.data$ProbCat[spatial.data$excProb&gt;=0.80 &amp; spatial.data$excProb&lt; 0.90] &lt;- 10 spatial.data$ProbCat[spatial.data$excProb&gt;=0.90 &amp; spatial.data$excProb&lt; 1.00] &lt;- 11 spatial.data$ProbCat[spatial.data$excProb == 1.00] &lt;- 12 # check to see if legend scheme is balanced table(spatial.data$ProbCat) Generating the probability map output: # map of exceedance probabilities tm_shape(spatial.data) + tm_fill(&quot;ProbCat&quot;, style = &quot;cat&quot;, title = &quot;Probability&quot;, palette = &quot;GnBu&quot;, labels = ProbCategorylist) + tm_shape(england_Region_shp) + tm_polygons(alpha = 0.05, border.col = &quot;black&quot;) + tm_text(&quot;name&quot;, size = &quot;AREA&quot;) + tm_layout(frame = FALSE, legend.outside = TRUE, legend.title.size = 0.8, legend.text.size = 0.7) + tm_compass(position = c(&quot;right&quot;, &quot;top&quot;)) + tm_scale_bar(position = c(&quot;right&quot;, &quot;bottom&quot;)) Output: Example Interpretation: We can see that the risk patterns for road accidents across England is quite heterogeneous. While it is quite pronounced in all 10 regions in England, the burden is quite significant in South West region with large numbers of local authorities having an increased risk which are statistically significant. While, there’s significant limitation the models used here - perhaps, the Department for Transport should do an investigation on these patterns starting with the South West area. 6.5 Task Try your hand on this problem in Stan: Build a spatial ICAR model using data on counts of low birth weights in Georgia US to create the following maps: Map showing the relative risk of low birth weight across the 163 counties in Georgia Map showing the statistical significance of the relative risk Map showing the Exceedance Probabilities using the threshold of RR &gt; 1 Use the following dataset: Low_birth_weights_data.csv: Contains NAME, Lowbirths (Counts) and ExpectedNumber Georgia_Shapefile.shp: Contains NAME "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
